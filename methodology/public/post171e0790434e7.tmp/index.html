<!doctype html><html lang=en><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.139.3"><title>- ExampleSite</title>
<meta property="og:title" content=" - ExampleSite"><link rel=stylesheet href=/css/fonts.css media=all><link rel=stylesheet href=/css/main.css media=all></head><body><div class=wrapper><header class=header><nav class=nav><a href=/ class=nav-logo><img src=/images/%3cnil%3e width height alt></a><ul class=nav-links><li><a href=/about/>About</a></li><li><a href=/categories/>categories</a></li><li><a href=/tags/>tags</a></li><li><a href=https://example.org>example.org</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title></h1><div class=article-content><div id=linear-algebra class="section level1" number=1><h1><span class=header-section-number>1</span> Linear Algebra</h1><p>Based on <span class=citation>(<strong>garrity_2007?</strong>)</span></p><div id=the-basic-vector-space-n class="section level2" number=1.1><h2><span class=header-section-number>1.1</span> The Basic Vector Space ( ^n )</h2><p>The quintessential vector space is ( ^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:</p><p><span class="math display">\[
(v_1, \ldots, v_n) + (w_1, \ldots, w_n) = (v_1 + w_1, \ldots, v_n + w_n)
\]</span></p><p>and to multiply each ( n )-tuple by a real number ( ):</p><p><span class="math display">\[
\lambda (v_1, \ldots, v_n) = (\lambda v_1, \ldots, \lambda v_n)
\]</span></p><p>In this way, each ( n )-tuple is commonly referred to as a vector, and the real numbers ( ) are known as scalars. When ( n = 2 ) or ( n = 3 ), this reduces to vectors in the plane and in space, which most of us learned in high school.</p><p>The natural relationship from ( ^n ) to ( ^m ) is established through matrix multiplication. We write a vector ( ^n ) as a column vector:</p><p><span class="math display">\[
\mathrm{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
\]</span></p><p>Similarly, we can write a vector in ( ^m ) as a column vector with ( m ) entries. Let ( A ) be an ( m n ) matrix:</p><p><span class="math display">\[
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
\]</span></p><p>Then, the product ( A ) is the ( m )-tuple:</p><p><span class="math display">\[
A \mathrm{x} = \begin{pmatrix}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\
\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n
\end{pmatrix}
\]</span></p><p>For any two vectors ( ) and ( ) in ( ^n ) and any two scalars ( ) and ( ), the following property holds:</p><p><span class="math display">\[
A (\lambda \mathrm{x} + \mu \mathrm{y}) = \lambda A \mathrm{x} + \mu A \mathrm{y}
\]</span></p><p>In the next section, we will use the linearity of matrix multiplication to motivate the definition of a linear transformation between vector spaces. Now, let’s relate all this to solving a system of linear equations. Suppose we are given numbers ( b_1, , b_m ) and numbers ( a_{ij}, , a_{mn} ). Our goal is to find ( n ) numbers ( x_1, , x_n ) that solve the following system of linear equations:</p><p><span class="math display">\[
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \\
&\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m
\end{aligned}
\]</span></p><p>Calculations in linear algebra often reduce to solving a system of linear equations. When there are only a few equations, we can find the solutions manually, but as the number of equations increases, the calculations quickly become less about pleasant algebraic manipulations and more about keeping track of many small individual details. In other words, it is an organizational problem.</p><p>We can write:</p><p><span class="math display">\[
\mathrm{b} = \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}
\]</span></p><p>and our unknowns as:</p><p><span class="math display">\[
\mathrm{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
\]</span></p><p>Then, we can rewrite our system of linear equations in the more visually appealing form:</p><p><span class="math display">\[
A \mathrm{x} = \mathrm{b}
\]</span></p><p>When ( m > n ) (when there are more equations than unknowns), we generally do not expect solutions. For example, when ( m = 3 ) and ( n = 2 ), this corresponds geometrically to the fact that three lines in a plane generally do not have a common intersection point. When ( m &lt; n ) (when there are more unknowns than equations), we generally expect there to be many solutions. In the case where ( m = 2 ) and ( n = 3 ), this corresponds geometrically to the fact that two planes in space generally intersect in an entire line. Much of the machinery of linear algebra deals with the remaining case when ( m = n ).</p><p>Therefore, we want to find the ( n ) column vector ( ) that solves ( A = ), where ( A ) is a given ( n n ) matrix and ( ) is a given ( n ) column vector.</p><p>Suppose the square matrix ( A ) has an inverse matrix ( A^{-1} ) (which means that ( A^{-1} ) is also ( n n ) and, more importantly, that ( A A^{-1} = I ), where ( I ) is the identity matrix). Then our solution will be:</p><p><span class="math display">\[
\mathrm{x} = A^{-1} \mathrm{b}
\]</span></p><p>because:</p><p><span class="math display">\[
A \mathrm{x} = A (A^{-1} \mathrm{b}) = I \mathrm{b} = \mathrm{b}
\]</span></p><p>Thus, solving our system of linear equations reduces to understanding when the ( n n ) matrix ( A ) has an inverse. (If an inverse matrix exists, then algorithms exist for its calculation). The key theorem of linear algebra, which is stated in section six, is essentially a list of many equivalencies for when an ( n n ) matrix has an inverse, and it is therefore crucial to understanding when a system of linear equations can be solved.</p></div><div id=vector-spaces-and-linear-transformations class="section level2" number=1.2><h2><span class=header-section-number>1.2</span> Vector Spaces and Linear Transformations</h2><p>The abstract approach to studying systems of linear equations begins with the concept of a vector space.</p><div class=definition><p>A set ( V ) is a <strong>vector space</strong> over the real numbers ( ) if there are two operations:</p><ol style=list-style-type:decimal><li><p><strong>Scalar multiplication</strong>: For every ( a ) and ( V ), there exists an element ( a V ), denoted ( a ), satisfying the properties of scalar multiplication.</p></li><li><p><strong>Vector addition</strong>: For every ( , V ), there exists an element ( + V ), denoted ( + ), satisfying the properties of vector addition.</p></li></ol><p>These operations must satisfy the following properties:</p><ul><li><strong>Additive identity</strong>: There exists an element ( V ) (the zero vector) such that ( + = ) for all ( V ).<br></li><li><strong>Additive inverse</strong>: For every ( V ), there exists an element ( - V ) such that ( + (-) = ).<br></li><li><strong>Commutativity</strong>: For all ( , V ), ( + = + ).<br></li><li><strong>Distributivity of scalar multiplication over vector addition</strong>: For all ( a ) and ( , V ), ( a( + ) = a + a ).<br></li><li><strong>Distributivity of scalar multiplication over scalar addition</strong>: For all ( a, b ) and ( V ), ( (a + b) = a + b ).<br></li><li><strong>Compatibility of scalar multiplication with field multiplication</strong>: For all ( a, b ) and ( V ), ( a(b) = (ab) ).<br></li><li><strong>Multiplicative identity</strong>: For all ( V ), ( 1 = ), where 1 is the multiplicative identity in ( ).<br></li></ul></div><p>The field of scalars can be replaced by other fields, such as ( ), without changing the general concept of vector spaces.</p><p>Elements of a vector space are referred to as <strong>vectors</strong>, while elements of the scalar field (e.g., ( )) are called <strong>scalars</strong>. For example, ( ^n ), the space of all ( n )-tuples of real numbers, is a vector space that satisfies these axioms.</p><p>The natural mappings between vector spaces are linear transformations.</p><div class=definition><p>A <strong>linear transformation</strong> ( T : V W ) is a function from a vector space ( V ) to a vector space ( W ) such that for all ( a_1, a_2 ) and ( _1, _2 V ):<br><span class="math display">\[
T(a_1 \mathrm{v}_1 + a_2 \mathrm{v}_2) = a_1 T(\mathrm{v}_1) + a_2 T(\mathrm{v}_2).
\]</span></p><p>An example of a linear transformation is matrix multiplication, mapping ( ^n ) to ( ^m ).</p></div><div class=definition><p>A subset ( U ) of a vector space ( V ) is called a <strong>subspace</strong> if ( U ) itself is a vector space under the operations of ( V ).</p><p>To determine whether a subset is a subspace, we use the following proposition:</p></div><div class=proposition><p>A subset ( U ) of a vector space ( V ) is a subspace if it is closed under addition and scalar multiplication.</p></div><p>Given a linear transformation ( T : V W ), we can naturally define two important subspaces of ( V ) and ( W ).</p><div class=definition><p>If ( T : V W ) is a linear transformation, then:</p><ul><li>The <strong>kernel</strong> of ( T ) is<br><span class="math display">\[
\ker(T) = \{ \mathrm{v} \in V : T(\mathrm{v}) = 0 \}.
\]</span><br></li><li>The <strong>image</strong> of ( T ) is<br><span class="math display">\[
\text{Im}(T) = \{ \mathrm{w} \in W : \text{there exists } \mathrm{v} \in V \text{ such that } T(\mathrm{v}) = \mathrm{w} \}.
\]</span></li></ul><p>The kernel of ( T ) is a subspace of ( V ), as closure under addition and scalar multiplication can be verified. Similarly, the image of ( T ) is a subspace of ( W ).</p></div><div id=an-example-beyond-finite-dimensions class="section level3" number=1.2.1><h3><span class=header-section-number>1.2.1</span> An Example Beyond Finite Dimensions</h3><p>If all vector spaces were finite-dimensional (like ( ^n )), the above abstraction would be trivial. However, vector spaces can also include function spaces.</p><p>Consider the set ( C^<em>[0,1] ) of all real-valued functions defined on ( [0,1] ) such that the ( k )-th derivative exists and is continuous. The sum of two such functions and the scalar multiplication of a function by a real number remain in ( C^</em>[0,1] ), making it a vector space. Unlike ( ^n ), ( C^*[0,1] ) is infinite-dimensional.</p><p>The derivative operator defines a linear transformation:<br><span class="math display">\[
\frac{d}{dx} : C^*[0,1] \to C^{k-1}[0,1].
\]</span></p><p>The kernel of this transformation is the set of functions whose ( k )-th derivative is zero, i.e., the set of constant functions.</p><p>Now consider the second-order differential equation:<br><span class="math display">\[
f'' + 3f' + 2f = 0.
\]</span></p><p>Let ( T ) be the linear transformation defined by:<br><span class="math display">\[
T(f) = f'' + 3f' + 2f.
\]</span></p><p>Finding a solution ( f(x) ) to the differential equation corresponds to finding an element in ( (T) ). This illustrates how the language of linear algebra provides tools for studying (linear) differential equations.</p></div></div><div id=bases-dimension-and-linear-transformations-as-matrices class="section level2" number=1.3><h2><span class=header-section-number>1.3</span> Bases, Dimension, and Linear Transformations as Matrices</h2><p>Our next goal is to define the dimension of a vector space.</p><div class=definition><p>A set of vectors ((v_1, , v_n)) forms a basis for the vector space (V) if, for any vector (v V), there exist unique scalars (a_1, , a_n ) such that</p><p>[
v = a_1v_1 + + a_nv_n.
]</p></div><div class=definition><p>The dimension of a vector space (V), denoted as ((V)), is the number of elements in a basis.</p></div><p>It is not obvious that the number of elements in a basis will always be the same, regardless of the chosen basis. To ensure that the definition of the dimension of a vector space is well-defined, we need the following theorem (which we will not prove):</p><div class=definition><p>All bases of a vector space (V) have the same number of elements.</p></div><p>For (^n), the usual basis is ({(1, 0, , 0), (0, 1, 0, , 0), , (0, , 0, 1)}). Therefore, (^n) has dimension (n). If this were not the case, the previous definition of dimension would be incorrect and we would need another. This is an example of the principle mentioned in the introduction. We have an intuitive understanding of what the dimension should mean for specific examples: a line should be one-dimensional, a plane two-dimensional, and three-dimensional space. We then formulate a precise definition. If this definition gives the “correct answer” for our three already understood examples, we are somewhat confident that the definition has captured what dimension means in this case. We can then apply the definition to examples where our intuitions fail.</p><p>Linked to the idea of a basis is:</p><div class=definition><p>The vectors ((v_1, , v_n)) in a vector space (V) are linearly independent if, whenever (a_1v_1 + + a_nv_n = 0), it must be the case that the scalars (a_1, , a_n) are all zero. Intuitively, a set of vectors is linearly independent if they all point in different directions. A basis, therefore, consists of a set of linearly independent vectors that span the vector space, where “span” means:</p></div><div class=definition><p>A set of vectors ((v_1, , v_n)) spans the vector space (V) if, for any vector (v V), there exist scalars (a_1, , a_n ) such that</p><p>[
v = a_1v_1 + + a_nv_n.
]</p></div><p>Our next goal is to show how all linear transformations (T: V W) between finite-dimensional spaces can be represented as matrix multiplication, provided that we fix bases for the vector spaces (V) and (W).</p><p>First, we fix a basis ({v_1, , v_n}) for (V) and a basis ({w_1, , w_m}) for (W). Before examining the linear transformation (T), we need to show how each element of the (n)-dimensional space (V) can be represented as a column vector in (^n) and how each element of the (m)-dimensional space (W) can be represented as a column vector in (^m). Given any vector (v V), by the definition of a basis, there exist unique real numbers (a_1, , a_n) such that:</p><p>[
v = a_1v_1 + + a_nv_n.
]</p><p>Thus, we represent the vector (v) with the column vector:</p>[
<span class="math display">\[\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}\]</span><p>.
]</p><p>Similarly, for any vector (w W), there exist unique real numbers (b_1, , b_m) such that:</p><p>[
w = b_1w_1 + + b_mw_m.
]</p><p>Here, we represent (w) as the column vector:</p>[
<span class="math display">\[\begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}\]</span><p>.
]</p><p>It is important to note that we have established a correspondence between the vectors in (V) and (W) and the column vectors in (^n) and (^m), respectively. More technically, we can prove that (V) is isomorphic to (^n) (which means there is a one-to-one and onto linear transformation from (V) to (^n)) and that (W) is isomorphic to (^m), although it must be emphasized that the real correspondence only exists after a basis has been chosen (which means that, while the isomorphism exists, it is not canonical; this is an important aspect because, in practice, we are often not provided with a basis).</p><p>Now, we want to represent a linear transformation (T: V W) as a matrix (A) of size (m n). For each basis vector (v_i) in the vector space (V), (T(v_i)) will be a vector in (W). Therefore, there will be real numbers (a_{ij}, , a_{im}) such that:</p><p>[
T(v_i) = a_{ij}w_1 + + a_{im}w_m.
]</p><p>We want to see that the linear transformation (T) corresponds to the matrix (A):</p>[
A =
<span class="math display">\[\begin{pmatrix} a_{11} & a_{21} & \dots & a_{m1} \\ a_{12} & a_{22} & \dots & a_{m2} \\ \vdots & \vdots & \ddots & \vdots \\ a_{1n} & a_{2n} & \dots & a_{mn} \end{pmatrix}\]</span><p>.
]</p><p>Given any vector (v V), with (v = a_1v_1 + + a_nv_n), we have:</p><p>[
T(v) = a_1T(v_1) + + a_nT(v_n) = a_1(a_{11}w_1 + + a_{1m}w_m) + + a_n(a_{n1}w_1 + + a_{nm}w_m).
]</p><p>Under the correspondences of the vector spaces with the respective column spaces, this can be seen as the matrix multiplication of (A) by the column vector corresponding to the vector (v):</p>[
A
<span class="math display">\[\begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix}\]</span>
=
<span class="math display">\[\begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{pmatrix}\]</span><p>.
]</p><p>It is important to note that if (T: V V) is a linear transformation of a vector space onto itself, then the corresponding matrix will be (n n), i.e., a square matrix.</p><p>Given different bases for the vector spaces (V) and (W), the matrix associated with the linear transformation (T) will change. A natural problem is to determine when two matrices actually represent the same linear transformation, but under different bases. This will be the subject of section seven.</p></div><div id=the-determinant class="section level2" number=1.4><h2><span class=header-section-number>1.4</span> The Determinant</h2><p>Our next task is to define the determinant of a matrix. In fact, we will present three alternative descriptions of the determinant. These descriptions are equivalent, and each has its own advantages.</p><p>The first method involves defining the determinant of a ( 1 ) matrix and then recursively defining the determinant of an ( n n ) matrix. Since ( 1 ) matrices are simply numbers, the following should not be surprising:</p><div class=definition><p>The determinant of a ( 1 ) matrix ( a ) is the real-valued function:<br><span class="math display">\[ \det(a) = a. \]</span></p></div><p>This should not yet seem significant.</p><p>Before defining the determinant for a general ( n n ) matrix, we need some notation. For an ( n n ) matrix:<br><span class="math display">\[ A =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix},
\]</span><br>we denote by ( A_{ij} ) the ( (n - 1) (n - 1) ) matrix obtained from ( A ) by removing the ( i )-th row and ( j )-th column. For example, if<br><span class="math display">\[ A =
\begin{pmatrix}
2 & 6 & 1 \\
3 & 4 & 8 \\
5 & 9 & 7
\end{pmatrix},
\]</span><br>then<br><span class="math display">\[ A_{23} =
\begin{pmatrix}
2 & 1 \\
3 & 8
\end{pmatrix}.
\]</span><br>Similarly, if<br><span class="math display">\[ A =
\begin{pmatrix}
6 & 7 \\
4 & 1 \\
9 & 8
\end{pmatrix},
\]</span><br>then<br><span class="math display">\[ A_{12} =
\begin{pmatrix}
7 & 8
\end{pmatrix}.
\]</span></p><p>Since we have a definition for the determinant of ( 1 ) matrices, we will assume by induction that we know the determinant of any ( (n - 1) (n - 1) ) matrix and use this to find the determinant of an ( n n ) matrix.</p><div class=definition><p>Let ( A ) be an ( n n ) matrix. The determinant of ( A ) is defined as:<br><span class="math display">\[ \det(A) = \sum_{k=1}^n a_{1k} \det(A_{1k}). \]</span></p></div><p>Thus, for<br><span class="math display">\[ A =
\begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix},
\]</span><br>we have:<br><span class="math display">\[ \det(A) = a_{11} \det(A_{11}) - a_{12} \det(A_{12}), \]</span><br>which aligns with the determinant formula most of us think of. The determinant of our earlier ( 3 ) matrix is:<br><span class="math display">\[ \det
\begin{pmatrix}
2 & 6 & 1 \\
3 & 4 & 8 \\
5 & 9 & 7
\end{pmatrix}
= 2 \det(A_{11}) - 3 \det(A_{12}) + 5 \det(A_{13}). \]</span></p><p>Although this definition is an efficient way to describe the determinant, it obscures many of its uses and intuitions.</p><p>The second way to describe the determinant incorporates its key algebraic properties. It highlights the functional properties of the determinant. Denote the ( n n ) matrix ( A ) as ( A = (A_1, A_2, , A_n) ), where ( A_i ) denotes the ( i )-th column:</p><div class=definition><p>The determinant of ( A ) is the unique real-valued function:<br><span class="math display">\[ \det: \text{Matrices} \to \mathbb{R}, \]</span><br>that satisfies:<br>1. ( (A_1, , c A_k, , A_n) = c (A_1, , A_k, , A_n) ).<br>2. ( (A_1, , A_k + A_{k’}, , A_n) = (A_1, , A_k, , A_n) ) for ( k k’ ).<br>3. ( () = 1. )</p></div><p>Thus, treating each column vector of a matrix as a vector in ( ^n ), the determinant can be viewed as a special function from ( ^n ^n ) to the real numbers.</p><p>To use this definition, one would need to prove that such a function on the space of matrices, satisfying these conditions, exists and is unique. Existence can be shown by verifying that our first (inductive) definition satisfies these conditions, although this involves tedious computation. The proof of uniqueness can be found in most linear algebra texts.</p><p>The third definition of the determinant is the most geometric but also the vaguest. Think of an ( n n ) matrix ( A ) as a linear transformation from ( ^n ) to ( ^n ). Then, ( A ) maps the unit cube in ( ^n ) to some other object (a parallelepiped). The unit cube has, by definition, a volume of one.</p><div class=definition><p>The determinant of the matrix ( A ) is the signed volume of the image of the unit cube.</p></div><p>This is not well-defined since the method of defining the volume of the image has not been described. In fact, most would define the signed volume of the image as the number given by the determinant using one of the two previous definitions. However, this can be made rigorous, albeit at the cost of losing much of the geometric intuition.</p><p>For example, the matrix<br><span class="math display">\[ A =
\begin{pmatrix}
2 & 0 \\
0 & 1
\end{pmatrix}
\]</span><br>maps the unit cube to a region with doubled area, so we must have:<br><span class="math display">\[ \det(A) = 2. \]</span></p><p>Signed volume means that if the orientations of the edges of the unit cube are reversed, we must have a negative sign for the volume. For example, consider the matrix<br><span class="math display">\[ A =
\begin{pmatrix}
0 & -1 \\
1 & 0
\end{pmatrix}. \]</span><br>Here, the image is:<br><span class="math display">\[ \begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}. \]</span><br>Note that the orientations of the sides are reversed. Since the area is still doubled, the definition forces:<br><span class="math display">\[ \det(A) = -2. \]</span></p><p>Defining orientation rigorously is somewhat complicated (we will do this in Chapter Six), but its meaning is straightforward.</p><p>The determinant has many algebraic properties. For example:</p><div class=lemma><p>If ( A ) and ( B ) are ( n n ) matrices, then:<br><span class="math display">\[ \det(AB) = \det(A) \cdot \det(B). \]</span></p></div><p>This can be proven via lengthy calculation or by focusing on the definition of the determinant as the change in volume of the unit cube.</p></div><div id=the-key-theorem-of-linear-algebra class="section level2" number=1.5><h2><span class=header-section-number>1.5</span> The Key Theorem of Linear Algebra</h2><p>Below is the fundamental theorem of linear algebra. <em>(Note: We have not yet defined eigenvalues or eigenvectors, but we will do so in Section 8.)</em></p><div class=theorem><p>(Key Theorem 1.6.1) Let ( A ) be an ( n n ) matrix. Then, the following statements are equivalent:<br>1. ( A ) is invertible.<br>2. ( (A) ).<br>3. ( (A) = {0} ).<br>4. If ( ) is a column vector in ( ^n ), there exists a unique column vector ( ) in ( ^n ) such that ( A = ).<br>5. The columns of ( A ) are linearly independent ( n ) column vectors.<br>6. The rows of ( A ) are linearly independent ( 1 n ) row vectors.<br>7. The transpose ( A^) of ( A ) is invertible. <em>(Here, if ( A = (a_{ij}) ), then ( A^= (a_{ji}) ).</em><br>8. All eigenvalues of ( A ) are nonzero.</p></div><p>We can restate this theorem in terms of linear transformations:</p><div class=theorem><p>(Key Theorem 1.6.2) Let ( T: V V ) be a linear transformation. Then, the following statements are equivalent:<br>1. ( T ) is invertible.<br>2. ( (T) ), where the determinant is defined via a choice of basis in ( V ).<br>3. ( (T) = {0} ).<br>4. If ( ) is a vector in ( V ), there exists a unique vector ( ) in ( V ) such that ( T() = ).<br>5. For any basis ( _1, , _n ) of ( V ), the image vectors ( T(_1), , T(_n) ) are linearly independent.<br>6. For any basis ( _1, , _n ) of ( V ), if ( S ) denotes the transpose linear transformation of ( T ), then the image vectors ( S(_1), , S(_n) ) are linearly independent.<br>7. The transpose ( T^) is invertible. <em>(Here, the transpose is defined via a choice of basis in ( V ).)</em><br>8. All eigenvalues of ( T ) are nonzero.</p></div><p>To clarify the correspondence between the two theorems, we note that we currently have definitions for the determinant and the transpose only for matrices, not for linear transformations. However, both notions can be extended to linear transformations by fixing a basis (or equivalently, by choosing an inner product, which will be defined in Chapter 13 on Fourier series).</p><p>It is important to note that while the actual value of ( (T) ) depends on the chosen basis, the condition ( (T) ) does not. Similar remarks apply to conditions (6) and (7).</p><p>An exercise (Exercise 7) encourages the reader to find any linear algebra textbook and complete the proof of this theorem. It is unlikely that the textbook will present the result in this exact form, making the act of translation itself part of the exercise’s purpose.</p><p>Each of these equivalences is significant and can be studied in its own right. It is remarkable that they are all equivalent.</p></div><div id=similar-matrices class="section level2" number=1.6><h2><span class=header-section-number>1.6</span> Similar Matrices</h2><p>Recall that given a coordinate system for a vector space ( V ) of dimension ( n ), we can represent a linear transformation ( T: V V ) as an ( n n ) matrix ( A ). However, if we choose a different coordinate system ( V’ ), the matrix representing the linear transformation ( T’ ) will generally differ from the original matrix ( A ). The goal of this section is to establish a clear criterion to determine when two matrices represent the same linear transformation under different choices of bases.</p><div class=definition><p>Two ( n n ) matrices ( A ) and ( B ) are <strong>similar</strong> if there exists an invertible matrix ( C ) such that<br><span class="math display">\[
A = C^{-1} B C.
\]</span></p></div><p>We aim to show that two matrices are similar precisely when they represent the same linear transformation. Let us choose two bases for the vector space ( V ), say ( {v_1, , v_n} ) (the ( v )-basis) and ( {w_1, , w_n} ) (the ( w )-basis). Let ( A ) be the matrix representing the linear transformation ( T ) with respect to the ( v )-basis, and ( B ) the matrix representing ( T ) with respect to the ( w )-basis. We seek to construct the matrix ( C ) such that<br><span class="math display">\[
A = C^{-1} B C.
\]</span></p><p>Recall that given the ( v )-basis, any vector ( V ) can be written as an ( n ) column vector as follows: there exist unique scalars ( a_1, , a_n ) such that<br><span class="math display">\[
\mathbf{z} = a_1 \mathbf{v}_1 + \dots + a_n \mathbf{v}_n.
\]</span></p><p>We then represent ( ) with respect to the ( v )-basis as the column vector:<br><span class="math display">\[
\begin{pmatrix}
a_1 \\
\vdots \\
a_n
\end{pmatrix}.
\]</span></p><p>Similarly, there exist unique scalars ( b_1, , b_n ) such that<br><span class="math display">\[
\mathbf{z} = b_1 \mathbf{w}_1 + \dots + b_n \mathbf{w}_n,
\]</span><br>which means that with respect to the ( w )-basis, the vector ( ) is represented as:<br><span class="math display">\[
\begin{pmatrix}
b_1 \\
\vdots \\
b_n
\end{pmatrix}.
\]</span></p><p>The desired matrix ( C ) is the one that satisfies the relationship:<br><span class="math display">\[
C A = B C.
\]</span></p><p>Determining when two matrices are similar is a result that frequently arises in mathematics and physics. Often, we need to select a coordinate system (a basis) to express anything, but the underlying mathematics or physics of interest is independent of the initial choice. The key question then becomes: <em>What is preserved when the coordinate system changes?</em> Similar matrices provide a starting point to understand these issues.</p></div><div id=eigenvalues-and-eigenvectors class="section level2" number=1.7><h2><span class=header-section-number>1.7</span> Eigenvalues and Eigenvectors</h2><p>In the previous section, we observed that two matrices represent the same linear transformation under different choices of bases precisely when they are similar. However, this does not tell us how to choose a basis for a vector space so that a linear transformation has a particularly simple matrix representation. For instance, the diagonal matrix<br><span class="math display">\[
A =
\begin{pmatrix}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 3
\end{pmatrix}
\]</span><br>is similar to the matrix<br><span class="math display">\[
B =
\begin{pmatrix}
1 & -5 & 15 \\
1 & -1 & 84 \\
-4 & -15 & 15
\end{pmatrix},
\]</span><br>but the simplicity of ( A ) is evident compared to ( B ). (Incidentally, it is not obvious that ( A ) and ( B ) are similar; I started with ( A ), chose a nonsingular matrix ( C ), and then used Mathematica software to compute ( C^{-1}AC ) to obtain ( B ). This was not immediately apparent but intentionally set up.)</p><p>One of the purposes of the following definitions of eigenvalues and eigenvectors is to provide tools for selecting good bases. However, there are many other reasons to understand eigenvalues and eigenvectors.</p><div class=definition><p>Let ( T: V V ) be a linear transformation. A nonzero vector ( V ) is an eigenvector of ( T ) with eigenvalue ( ), a scalar, if<br><span class="math display">\[
T(\mathbf{v}) = \lambda \mathbf{v}.
\]</span><br>For an ( n n ) matrix ( A ), a nonzero column vector ( ^n ) is an eigenvector with eigenvalue ( ), a scalar, if<br><span class="math display">\[
A \mathbf{x} = \lambda \mathbf{x}.
\]</span></p></div><p>Geometrically, a vector ( ) is an eigenvector of the linear transformation ( T ) with eigenvalue ( ) if ( T ) stretches ( ) by a factor of ( ).</p><p>For example, consider the matrix<br><span class="math display">\[
\begin{pmatrix}
-2 & 1 \\
2 & 7
\end{pmatrix}.
\]</span><br>Here, ( = 2 ) is an eigenvalue, and ( ) is an eigenvector for the linear transformation represented by this ( 2 ) matrix.</p><p>Fortunately, there is a simple way to describe the eigenvalues of a square matrix, allowing us to see that the eigenvalues of a matrix are preserved under a similarity transformation.</p><div class=definition><p>A scalar ( ) is an eigenvalue of a square matrix ( A ) if and only if ( ) is a root of the polynomial<br><span class="math display">\[
P(t) = \det(tI - A),
\]</span><br>where ( P(t) ) is called the characteristic polynomial of ( A ).</p></div><p><strong>Proof</strong>: Suppose ( ) is an eigenvalue of ( A ), with eigenvector ( ). Then,<br><span class="math display">\[
A \mathbf{v} = \lambda \mathbf{v},
\]</span><br>which implies<br><span class="math display">\[
A \mathbf{v} - \lambda \mathbf{v} = 0.
\]</span><br>Introducing the identity matrix ( I ), we rewrite this as<br><span class="math display">\[
0 = (\lambda I - A) \mathbf{v}.
\]</span><br>Thus, the matrix ( I - A ) has a nontrivial kernel, ( ). By the key theorem of linear algebra, this occurs precisely when<br><span class="math display">\[
\det(\lambda I - A) = 0,
\]</span><br>which means ( ) is a root of the characteristic polynomial ( P(t) = (tI - A) ). Since these implications are reversible, the theorem is established.</p><div class=theorem><p>If ( A ) and ( B ) are similar matrices, then the characteristic polynomial of ( A ) is equal to the characteristic polynomial of ( B ).</p></div><p><strong>Proof</strong>: For ( A ) and ( B ) to be similar, there must exist an invertible matrix ( C ) such that ( A = C^{-1}BC ). Then,<br><span class="math display">\[
\det(tI - A) = \det(tI - C^{-1}BC) = \det(C^{-1}(tI - B)C) = \det(C^{-1})\det(tI - B)\det(C) = \det(tI - B),
\]</span><br>using the property ( (C^{-1}C) = 1 = (C^{-1})(C) ).</p><p>Since similar matrices have the same characteristic polynomial, it follows that their eigenvalues must also be the same.</p><div class=corollary><p>The eigenvalues of similar matrices are identical. Thus, to determine if two matrices are not similar, check whether their eigenvalues differ. If they do, the matrices are not similar.</p><p>However, in general, identical eigenvalues do not imply that matrices are similar. For example, the matrices<br><span class="math display">\[
A =
\begin{pmatrix}
4 & 2 \\
7 & 5
\end{pmatrix}
\quad \text{and} \quad
B =
\begin{pmatrix}
2 & 3 \\
5 & 4
\end{pmatrix}
\]</span><br>have eigenvalues ( 1 ) and ( 2 ), but they are not similar. (This can be shown by assuming the existence of an invertible matrix ( C ) such that ( C^{-1}AC = B ) and then demonstrating that ( (C) = 0 ), which contradicts the invertibility of ( C ).)</p></div><div id=eigenvalues-and-determinants class="section level3" number=1.7.1><h3><span class=header-section-number>1.7.1</span> Eigenvalues and Determinants</h3><p>Since the characteristic polynomial ( P(t) ) does not change under a similarity transformation, the coefficients of ( P(t) ) are also preserved. As these coefficients are polynomial functions (complex) of the entries of the matrix ( A ), we obtain special polynomials of the entries of ( A ) that are invariant under similarity transformations. One such coefficient, already seen in another form, is the determinant of ( A ), as highlighted in the following theorem. This theorem establishes a deeper connection between the eigenvalues of ( A ) and the determinant of ( A ).</p><div class=theorem><p>Let ( _1, _2, , _n ) be the eigenvalues (counted with multiplicity) of a matrix ( A ). Then,<br><span class="math display">\[
\det(A) = \lambda_1 \cdot \lambda_2 \cdot \dots \cdot \lambda_n.
\]</span></p></div><p>Before proving this theorem, we need to clarify the concept of counting eigenvalues “with multiplicity.” The difficulty arises because a polynomial can have a root that must be counted more than once. For example, the polynomial ( (z - 2)^2 ) has a single root ( 2 ), but we count it twice. This situation is particularly relevant for the characteristic polynomial.</p><p>For instance, consider the matrix<br><span class="math display">\[
A =
\begin{pmatrix}
5 & 0 & 0 \\
0 & 5 & 0 \\
0 & 0 & 4
\end{pmatrix},
\]</span><br>which has the characteristic polynomial ( P(t) = (t - 5)(t - 5)(t - 4) ). Here, we list the eigenvalues as ( 4, 5, ) and ( 5 ), counting the eigenvalue ( 5 ) twice.</p><p><strong>Proof</strong>: Since the eigenvalues ( _1, _2, , _n ) are the roots of the characteristic polynomial ( (tI - A) ), we can write<br><span class="math display">\[
\det(tI - A) = (t - \lambda_1)(t - \lambda_2) \cdots (t - \lambda_n).
\]</span><br>Setting ( t = 0 ), we obtain<br><span class="math display">\[
(-1)^n \lambda_1 \lambda_2 \cdots \lambda_n = \det(-A).
\]</span><br>In the matrix ( -A ), each column of ( A ) is multiplied by ( -1 ). Using the linearity of the determinant, we can factor out ( -1 ) from each column, yielding<br><span class="math display">\[
\det(-A) = (-1)^n \det(A).
\]</span><br>Substituting, we have<br><span class="math display">\[
(-1)^n \lambda_1 \lambda_2 \cdots \lambda_n = (-1)^n \det(A),
\]</span><br>and thus,<br><span class="math display">\[
\det(A) = \lambda_1 \cdot \lambda_2 \cdots \lambda_n.
\]</span></p><p>To find a “good” basis for representing a linear transformation, we measure “goodness” by how close the matrix is to being diagonal. Here, we focus on a special but common class of matrices: symmetric matrices. A matrix ( A ) is symmetric if ( A = (a_{ij}) ) satisfies ( a_{ij} = a_{ji} ), meaning the element in the ( i )-th row and ( j )-th column equals the element in the ( j )-th row and ( i )-th column.</p><p>For example:<br><span class="math display">\[
\begin{pmatrix}
5 & 3 & 4 \\
3 & 5 & 2 \\
4 & 2 & 4
\end{pmatrix}
\]</span><br>is symmetric, but<br><span class="math display">\[
\begin{pmatrix}
5 & 6 & 2 \\
2 & 5 & 18 \\
3 & 3 & 4
\end{pmatrix}
\]</span><br>is not.</p><div class=theorem><p>If ( A ) is a symmetric matrix, then there exists a matrix ( B ) similar to ( A ) that is diagonal. Moreover, the entries along the diagonal of ( B ) are precisely the eigenvalues of ( A ).</p></div><p><strong>Proof</strong>: The proof relies on showing that the eigenvectors of ( A ) form a basis such that ( A ) becomes diagonal in this basis. We assume that ( A )’s eigenvalues are distinct since technical difficulties arise when eigenvalues have multiplicity.</p><p>Let ( v_1, v_2, , v_n ) be the eigenvectors of ( A ) with corresponding eigenvalues ( _1, _2, , _n ). Form the matrix<br><span class="math display">\[
C =
\begin{pmatrix}
v_1 & v_2 & \dots & v_n
\end{pmatrix},
\]</span><br>where the ( i )-th column of ( C ) is the column vector ( v_i ). We will show that the matrix ( C^{-1}AC ) satisfies our theorem. Specifically, we aim to demonstrate<br><span class="math display">\[
C^{-1}AC = B,
\]</span><br>where ( B ) is the desired diagonal matrix.</p><p>Define ( B ) as<br><span class="math display">\[
B =
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}.
\]</span><br>The diagonal matrix ( B ) is the only matrix satisfying ( Be_i = _i e_i ) for all ( i ), where ( e_i ) is the standard basis vector. Observe that ( Ce_i = v_i ) for all ( i ). Thus,<br><span class="math display">\[
C^{-1}ACe_i = C^{-1}Av_i = C^{-1}(\lambda_i v_i) = \lambda_i C^{-1}v_i = \lambda_i e_i,
\]</span><br>proving that<br><span class="math display">\[
C^{-1}AC = B.
\]</span></p><p>This result is not the end of the story. For nonsymmetric matrices, there are other canonical forms to find “good” similar matrices, such as the Jordan canonical form, upper triangular form, and rational canonical form.</p></div></div></div></div></article></main><footer class=footer><ul class=footer-links><li><a href=https://gohugo.io/ class=footer-links-kudos>Made with <img src=/images/hugo-logo.png alt="Img link to Hugo website" width=22 height=22></a></li></ul></footer></div><script src=/js/math-code.js></script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>