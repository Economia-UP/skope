<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Linear Algebra | ExampleSite</title>
<meta name=keywords content><meta name=description content="Based on @garrity_2007
The Basic Vector Space ( = \mathbb{R}^n )
The quintessential vector space is (R^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:
$$
(v_1, \ldots, v_n) + (w_1, \ldots, w_n) = (v_1 + w_1, \ldots, v_n + w_n)
$$"><meta name=author content="Juan Alvaro Diaz Raimond Kedilhac"><link rel=canonical href=https://jadrk040507.github.io/skope/methodology/public/post/2024-12-11-linear-algebra/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/skope/methodology/public/assets/css/stylesheet.d72444526d7ecbdb0015438a7fa89054a658bf759d0542e2e5df81ce94b493ee.css integrity="sha256-1yREUm1+y9sAFUOKf6iQVKZYv3WdBULi5d+BzpS0k+4=" rel="preload stylesheet" as=style><link rel=icon href=https://jadrk040507.github.io/skope/methodology/public/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://jadrk040507.github.io/skope/methodology/public/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://jadrk040507.github.io/skope/methodology/public/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://jadrk040507.github.io/skope/methodology/public/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://jadrk040507.github.io/skope/methodology/public/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://jadrk040507.github.io/skope/methodology/public/post/2024-12-11-linear-algebra/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.15/dist/katex.min.css integrity=sha384-Htz9HMhiwV8GuQ28Xr9pEs1B4qJiYu/nYLLwlDklR53QibDfmQzi7rYxXhMH/5/u crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.15/dist/katex.min.js integrity=sha384-bxmi2jLGCvnsEqMuYLKE/KsVCxV3PqmKeK6Y6+lmNXBry6+luFkEOsmp5vD9I/7+ crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.15/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"(",right:")",display:!1},{left:"[",right:"]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://jadrk040507.github.io/skope/methodology/public/post/2024-12-11-linear-algebra/"><meta property="og:site_name" content="ExampleSite"><meta property="og:title" content="Linear Algebra"><meta property="og:description" content="Based on @garrity_2007
The Basic Vector Space ( = \mathbb{R}^n ) The quintessential vector space is (R^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:
$$ (v_1, \ldots, v_n) + (w_1, \ldots, w_n) = (v_1 + w_1, \ldots, v_n + w_n) $$"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-12-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-11T00:00:00+00:00"><meta property="og:image" content="https://jadrk040507.github.io/skope/methodology/public/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jadrk040507.github.io/skope/methodology/public/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Linear Algebra"><meta name=twitter:description content="Based on @garrity_2007
The Basic Vector Space ( = \mathbb{R}^n )
The quintessential vector space is (R^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:
$$
(v_1, \ldots, v_n) + (w_1, \ldots, w_n) = (v_1 + w_1, \ldots, v_n + w_n)
$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://jadrk040507.github.io/skope/methodology/public/post/"},{"@type":"ListItem","position":2,"name":"Linear Algebra","item":"https://jadrk040507.github.io/skope/methodology/public/post/2024-12-11-linear-algebra/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Linear Algebra","name":"Linear Algebra","description":"Based on @garrity_2007\nThe Basic Vector Space ( = \\mathbb{R}^n ) The quintessential vector space is (R^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:\n$$ (v_1, \\ldots, v_n) + (w_1, \\ldots, w_n) = (v_1 + w_1, \\ldots, v_n + w_n) $$\n","keywords":[],"articleBody":"Based on @garrity_2007\nThe Basic Vector Space ( = \\mathbb{R}^n ) The quintessential vector space is (R^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:\n$$ (v_1, \\ldots, v_n) + (w_1, \\ldots, w_n) = (v_1 + w_1, \\ldots, v_n + w_n) $$\nand to multiply each ( n )-tuple by a real number ( \\lambda ):\n$$ \\lambda (v_1, \\ldots, v_n) = (\\lambda v_1, \\ldots, \\lambda v_n) $$\nIn this way, each ( n )-tuple is commonly referred to as a vector, and the real numbers ( \\lambda ) are known as scalars. When ( n = 2 ) or ( n = 3 ), this reduces to vectors in the plane and in space, which most of us learned in high school.\nThe natural relationship from ( \\mathbb{R}^n ) to ( \\mathbb{R}^m ) is established through matrix multiplication. We write a vector ( \\mathrm{x} \\in \\mathbb{R}^n ) as a column vector:\n$$ \\mathrm{x} = \\begin{pmatrix} x_1 \\ x_2 \\ \\vdots \\ x_n \\end{pmatrix} $$\nSimilarly, we can write a vector in ( \\mathbb{R}^m ) as a column vector with ( m ) entries. Let ( A ) be an ( m \\times n ) matrix:\n$$ A = \\begin{pmatrix} a_{11} \u0026 a_{12} \u0026 \\cdots \u0026 a_{1n} \\ a_{21} \u0026 a_{22} \u0026 \\cdots \u0026 a_{2n} \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ a_{m1} \u0026 a_{m2} \u0026 \\cdots \u0026 a_{mn} \\end{pmatrix} $$\nThen, the product ( A \\mathrm{x} ) is the ( m )-tuple:\n$$ A \\mathrm{x} = \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n \\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \\ \\vdots \\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n \\end{pmatrix} $$\nFor any two vectors ( \\mathrm{x} ) and ( \\mathrm{y} ) in ( \\mathbb{R}^n ) and any two scalars ( \\lambda ) and ( \\mu ), the following property holds:\n$$ A (\\lambda \\mathrm{x} + \\mu \\mathrm{y}) = \\lambda A \\mathrm{x} + \\mu A \\mathrm{y} $$\nIn the next section, we will use the linearity of matrix multiplication to motivate the definition of a linear transformation between vector spaces. Now, let’s relate all this to solving a system of linear equations. Suppose we are given numbers ( b_1, \\ldots, b_m ) and numbers ( a_{ij}, \\ldots, a_{mn} ). Our goal is to find ( n ) numbers ( x_1, \\ldots, x_n ) that solve the following system of linear equations:\n$$ \\begin{aligned} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n \u0026= b_1 \\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \u0026= b_2 \\ \u0026\\vdots \\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n \u0026= b_m \\end{aligned} $$\nCalculations in linear algebra often reduce to solving a system of linear equations. When there are only a few equations, we can find the solutions manually, but as the number of equations increases, the calculations quickly become less about pleasant algebraic manipulations and more about keeping track of many small individual details. In other words, it is an organizational problem.\nWe can write:\n$$ \\mathrm{b} = \\begin{pmatrix} b_1 \\ b_2 \\ \\vdots \\ b_m \\end{pmatrix} $$\nand our unknowns as:\n$$ \\mathrm{x} = \\begin{pmatrix} x_1 \\ x_2 \\ \\vdots \\ x_n \\end{pmatrix} $$\nThen, we can rewrite our system of linear equations in the more visually appealing form:\n$$ A \\mathrm{x} = \\mathrm{b} $$\nWhen ( m \u003e n ) (when there are more equations than unknowns), we generally do not expect solutions. For example, when ( m = 3 ) and ( n = 2 ), this corresponds geometrically to the fact that three lines in a plane generally do not have a common intersection point. When ( m \u003c n ) (when there are more unknowns than equations), we generally expect there to be many solutions. In the case where ( m = 2 ) and ( n = 3 ), this corresponds geometrically to the fact that two planes in space generally intersect in an entire line. Much of the machinery of linear algebra deals with the remaining case when ( m = n ).\nTherefore, we want to find the ( n \\times 1 ) column vector ( \\mathrm{x} ) that solves ( A \\mathrm{x} = \\mathrm{b} ), where ( A ) is a given ( n \\times n ) matrix and ( \\mathrm{b} ) is a given ( n \\times 1 ) column vector.\nSuppose the square matrix ( A ) has an inverse matrix ( A^{-1} ) (which means that ( A^{-1} ) is also ( n \\times n ) and, more importantly, that ( A A^{-1} = I ), where ( I ) is the identity matrix). Then our solution will be:\n$$ \\mathrm{x} = A^{-1} \\mathrm{b} $$\nbecause:\n$$ A \\mathrm{x} = A (A^{-1} \\mathrm{b}) = I \\mathrm{b} = \\mathrm{b} $$\nThus, solving our system of linear equations reduces to understanding when the ( n \\times n ) matrix ( A ) has an inverse. (If an inverse matrix exists, then algorithms exist for its calculation). The key theorem of linear algebra, which is stated in section six, is essentially a list of many equivalencies for when an ( n \\times n ) matrix has an inverse, and it is therefore crucial to understanding when a system of linear equations can be solved.\nVector Spaces and Linear Transformations The abstract approach to studying systems of linear equations begins with the concept of a vector space.\n:::{.definition}\nA set ( V ) is a vector space over the real numbers ( \\mathbb{R} ) if there are two operations:\nScalar multiplication: For every ( a \\in \\mathbb{R} ) and ( \\mathrm{v} \\in V ), there exists an element ( a \\cdot \\mathrm{v} \\in V ), denoted ( a\\mathrm{v} ), satisfying the properties of scalar multiplication.\nVector addition: For every ( \\mathrm{v}, \\mathrm{w} \\in V ), there exists an element ( \\mathrm{v} + \\mathrm{w} \\in V ), denoted ( \\mathrm{v} + \\mathrm{w} ), satisfying the properties of vector addition.\nThese operations must satisfy the following properties:\nAdditive identity: There exists an element ( \\mathrm{0} \\in V ) (the zero vector) such that ( \\mathrm{0} + \\mathrm{v} = \\mathrm{v} ) for all ( \\mathrm{v} \\in V ). Additive inverse: For every ( \\mathrm{v} \\in V ), there exists an element ( -\\mathrm{v} \\in V ) such that ( \\mathrm{v} + (-\\mathrm{v}) = \\mathrm{0} ). Commutativity: For all ( \\mathrm{v}, \\mathrm{w} \\in V ), ( \\mathrm{v} + \\mathrm{w} = \\mathrm{w} + \\mathrm{v} ). Distributivity of scalar multiplication over vector addition: For all ( a \\in \\mathbb{R} ) and ( \\mathrm{v}, \\mathrm{w} \\in V ), ( a(\\mathrm{v} + \\mathrm{w}) = a\\mathrm{v} + a\\mathrm{w} ). Distributivity of scalar multiplication over scalar addition: For all ( a, b \\in \\mathbb{R} ) and ( \\mathrm{v} \\in V ), ( (a + b)\\mathrm{v} = a\\mathrm{v} + b\\mathrm{v} ). Compatibility of scalar multiplication with field multiplication: For all ( a, b \\in \\mathbb{R} ) and ( \\mathrm{v} \\in V ), ( a(b\\mathrm{v}) = (ab)\\mathrm{v} ). Multiplicative identity: For all ( \\mathrm{v} \\in V ), ( 1 \\cdot \\mathrm{v} = \\mathrm{v} ), where 1 is the multiplicative identity in ( \\mathbb{R} ).\n::: The field of scalars can be replaced by other fields, such as ( \\mathrm{C} ), without changing the general concept of vector spaces.\nElements of a vector space are referred to as vectors, while elements of the scalar field (e.g., ( \\mathbb{R} )) are called scalars. For example, ( \\mathbb{R}^n ), the space of all ( n )-tuples of real numbers, is a vector space that satisfies these axioms.\nThe natural mappings between vector spaces are linear transformations.\n:::{.definition}\nA linear transformation ( T : V \\to W ) is a function from a vector space ( V ) to a vector space ( W ) such that for all ( a_1, a_2 \\in \\mathbb{R} ) and ( \\mathrm{v}_1, \\mathrm{v}_2 \\in V ):\n$$ T(a_1 \\mathrm{v}_1 + a_2 \\mathrm{v}_2) = a_1 T(\\mathrm{v}_1) + a_2 T(\\mathrm{v}_2). $$\nAn example of a linear transformation is matrix multiplication, mapping ( \\mathbb{R}^n ) to ( \\mathbb{R}^m ).\n:::\n:::{.definition}\nA subset ( U ) of a vector space ( V ) is called a subspace if ( U ) itself is a vector space under the operations of ( V ).\nTo determine whether a subset is a subspace, we use the following proposition:\n:::\n:::{.proposition}\nA subset ( U ) of a vector space ( V ) is a subspace if it is closed under addition and scalar multiplication.\n:::\nGiven a linear transformation ( T : V \\to W ), we can naturally define two important subspaces of ( V ) and ( W ).\n:::{.definition}\nIf ( T : V \\to W ) is a linear transformation, then:\nThe kernel of ( T ) is\n$$ \\ker(T) = { \\mathrm{v} \\in V : T(\\mathrm{v}) = 0 }. $$ The image of ( T ) is\n$$ \\text{Im}(T) = { \\mathrm{w} \\in W : \\text{there exists } \\mathrm{v} \\in V \\text{ such that } T(\\mathrm{v}) = \\mathrm{w} }. $$ The kernel of ( T ) is a subspace of ( V ), as closure under addition and scalar multiplication can be verified. Similarly, the image of ( T ) is a subspace of ( W ).\n:::\nAn Example Beyond Finite Dimensions If all vector spaces were finite-dimensional (like ( \\mathbb{R}^n )), the above abstraction would be trivial. However, vector spaces can also include function spaces.\nConsider the set ( C^[0,1] ) of all real-valued functions defined on ( [0,1] ) such that the ( k )-th derivative exists and is continuous. The sum of two such functions and the scalar multiplication of a function by a real number remain in ( C^[0,1] ), making it a vector space. Unlike ( \\mathbb{R}^n ), ( C^*[0,1] ) is infinite-dimensional.\nThe derivative operator defines a linear transformation:\n$$ \\frac{d}{dx} : C^*[0,1] \\to C^{k-1}[0,1]. $$\nThe kernel of this transformation is the set of functions whose ( k )-th derivative is zero, i.e., the set of constant functions.\nNow consider the second-order differential equation:\n$$ f’’ + 3f’ + 2f = 0. $$\nLet ( T ) be the linear transformation defined by:\n$$ T(f) = f’’ + 3f’ + 2f. $$\nFinding a solution ( f(x) ) to the differential equation corresponds to finding an element in ( \\ker(T) ). This illustrates how the language of linear algebra provides tools for studying (linear) differential equations.\nBases, Dimension, and Linear Transformations as Matrices Our next goal is to define the dimension of a vector space.\n:::{.definition} A set of vectors ((v_1, \\dots, v_n)) forms a basis for the vector space (V) if, for any vector (v \\in V), there exist unique scalars (a_1, \\dots, a_n \\in \\mathbb{R}) such that\n[ v = a_1v_1 + \\dots + a_nv_n. ] :::\n:::{.definition} The dimension of a vector space (V), denoted as (\\text{dim}(V)), is the number of elements in a basis. :::\nIt is not obvious that the number of elements in a basis will always be the same, regardless of the chosen basis. To ensure that the definition of the dimension of a vector space is well-defined, we need the following theorem (which we will not prove):\n:::{.definition} All bases of a vector space (V) have the same number of elements. :::\nFor (\\mathbb{R}^n), the usual basis is ({(1, 0, \\dots, 0), (0, 1, 0, \\dots, 0), \\dots, (0, \\dots, 0, 1)}). Therefore, (\\mathbb{R}^n) has dimension (n). If this were not the case, the previous definition of dimension would be incorrect and we would need another. This is an example of the principle mentioned in the introduction. We have an intuitive understanding of what the dimension should mean for specific examples: a line should be one-dimensional, a plane two-dimensional, and three-dimensional space. We then formulate a precise definition. If this definition gives the “correct answer” for our three already understood examples, we are somewhat confident that the definition has captured what dimension means in this case. We can then apply the definition to examples where our intuitions fail.\nLinked to the idea of a basis is:\n:::{.definition} The vectors ((v_1, \\dots, v_n)) in a vector space (V) are linearly independent if, whenever (a_1v_1 + \\dots + a_nv_n = 0), it must be the case that the scalars (a_1, \\dots, a_n) are all zero. Intuitively, a set of vectors is linearly independent if they all point in different directions. A basis, therefore, consists of a set of linearly independent vectors that span the vector space, where “span” means: :::\n:::{.definition} A set of vectors ((v_1, \\dots, v_n)) spans the vector space (V) if, for any vector (v \\in V), there exist scalars (a_1, \\dots, a_n \\in \\mathbb{R}) such that\n[ v = a_1v_1 + \\dots + a_nv_n. ] :::\nOur next goal is to show how all linear transformations (T: V \\to W) between finite-dimensional spaces can be represented as matrix multiplication, provided that we fix bases for the vector spaces (V) and (W).\nFirst, we fix a basis ({v_1, \\dots, v_n}) for (V) and a basis ({w_1, \\dots, w_m}) for (W). Before examining the linear transformation (T), we need to show how each element of the (n)-dimensional space (V) can be represented as a column vector in (\\mathbb{R}^n) and how each element of the (m)-dimensional space (W) can be represented as a column vector in (\\mathbb{R}^m). Given any vector (v \\in V), by the definition of a basis, there exist unique real numbers (a_1, \\dots, a_n) such that:\n[ v = a_1v_1 + \\dots + a_nv_n. ]\nThus, we represent the vector (v) with the column vector:\n[ \\begin{pmatrix} a_1 \\ a_2 \\ \\vdots \\ a_n \\end{pmatrix}. ]\nSimilarly, for any vector (w \\in W), there exist unique real numbers (b_1, \\dots, b_m) such that:\n[ w = b_1w_1 + \\dots + b_mw_m. ]\nHere, we represent (w) as the column vector:\n[ \\begin{pmatrix} b_1 \\ b_2 \\ \\vdots \\ b_m \\end{pmatrix}. ]\nIt is important to note that we have established a correspondence between the vectors in (V) and (W) and the column vectors in (\\mathbb{R}^n) and (\\mathbb{R}^m), respectively. More technically, we can prove that (V) is isomorphic to (\\mathbb{R}^n) (which means there is a one-to-one and onto linear transformation from (V) to (\\mathbb{R}^n)) and that (W) is isomorphic to (\\mathbb{R}^m), although it must be emphasized that the real correspondence only exists after a basis has been chosen (which means that, while the isomorphism exists, it is not canonical; this is an important aspect because, in practice, we are often not provided with a basis).\nNow, we want to represent a linear transformation (T: V \\to W) as a matrix (A) of size (m \\times n). For each basis vector (v_i) in the vector space (V), (T(v_i)) will be a vector in (W). Therefore, there will be real numbers (a_{ij}, \\dots, a_{im}) such that:\n[ T(v_i) = a_{ij}w_1 + \\dots + a_{im}w_m. ]\nWe want to see that the linear transformation (T) corresponds to the matrix (A):\n[ A = \\begin{pmatrix} a_{11} \u0026 a_{21} \u0026 \\dots \u0026 a_{m1} \\ a_{12} \u0026 a_{22} \u0026 \\dots \u0026 a_{m2} \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ a_{1n} \u0026 a_{2n} \u0026 \\dots \u0026 a_{mn} \\end{pmatrix}. ]\nGiven any vector (v \\in V), with (v = a_1v_1 + \\dots + a_nv_n), we have:\n[ T(v) = a_1T(v_1) + \\dots + a_nT(v_n) = a_1(a_{11}w_1 + \\dots + a_{1m}w_m) + \\dots + a_n(a_{n1}w_1 + \\dots + a_{nm}w_m). ]\nUnder the correspondences of the vector spaces with the respective column spaces, this can be seen as the matrix multiplication of (A) by the column vector corresponding to the vector (v):\n[ A \\begin{pmatrix} a_1 \\ a_2 \\ \\vdots \\ a_n \\end{pmatrix} = \\begin{pmatrix} b_1 \\ b_2 \\ \\vdots \\ b_m \\end{pmatrix}. ]\nIt is important to note that if (T: V \\to V) is a linear transformation of a vector space onto itself, then the corresponding matrix will be (n \\times n), i.e., a square matrix.\nGiven different bases for the vector spaces (V) and (W), the matrix associated with the linear transformation (T) will change. A natural problem is to determine when two matrices actually represent the same linear transformation, but under different bases. This will be the subject of section seven.\nThe Determinant Our next task is to define the determinant of a matrix. In fact, we will present three alternative descriptions of the determinant. These descriptions are equivalent, and each has its own advantages.\nThe first method involves defining the determinant of a ( 1 \\times 1 ) matrix and then recursively defining the determinant of an ( n \\times n ) matrix. Since ( 1 \\times 1 ) matrices are simply numbers, the following should not be surprising:\n:::{.definition}\nThe determinant of a ( 1 \\times 1 ) matrix ( a ) is the real-valued function:\n$$ \\det(a) = a. $$\n:::\nThis should not yet seem significant.\nBefore defining the determinant for a general ( n \\times n ) matrix, we need some notation. For an ( n \\times n ) matrix:\n$$ A = \\begin{pmatrix} a_{11} \u0026 a_{12} \u0026 \\cdots \u0026 a_{1n} \\ a_{21} \u0026 a_{22} \u0026 \\cdots \u0026 a_{2n} \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ a_{n1} \u0026 a_{n2} \u0026 \\cdots \u0026 a_{nn} \\end{pmatrix}, $$\nwe denote by ( A_{ij} ) the ( (n - 1) \\times (n - 1) ) matrix obtained from ( A ) by removing the ( i )-th row and ( j )-th column. For example, if\n$$ A = \\begin{pmatrix} 2 \u0026 6 \u0026 1 \\ 3 \u0026 4 \u0026 8 \\ 5 \u0026 9 \u0026 7 \\end{pmatrix}, $$\nthen\n$$ A_{23} = \\begin{pmatrix} 2 \u0026 1 \\ 3 \u0026 8 \\end{pmatrix}. $$\nSimilarly, if\n$$ A = \\begin{pmatrix} 6 \u0026 7 \\ 4 \u0026 1 \\ 9 \u0026 8 \\end{pmatrix}, $$\nthen\n$$ A_{12} = \\begin{pmatrix} 7 \u0026 8 \\end{pmatrix}. $$\nSince we have a definition for the determinant of ( 1 \\times 1 ) matrices, we will assume by induction that we know the determinant of any ( (n - 1) \\times (n - 1) ) matrix and use this to find the determinant of an ( n \\times n ) matrix.\n:::{.definition}\nLet ( A ) be an ( n \\times n ) matrix. The determinant of ( A ) is defined as:\n$$ \\det(A) = \\sum_{k=1}^n a_{1k} \\det(A_{1k}). $$\n:::\nThus, for\n$$ A = \\begin{pmatrix} a_{11} \u0026 a_{12} \\ a_{21} \u0026 a_{22} \\end{pmatrix}, $$\nwe have:\n$$ \\det(A) = a_{11} \\det(A_{11}) - a_{12} \\det(A_{12}), $$\nwhich aligns with the determinant formula most of us think of. The determinant of our earlier ( 3 \\times 3 ) matrix is:\n$$ \\det \\begin{pmatrix} 2 \u0026 6 \u0026 1 \\ 3 \u0026 4 \u0026 8 \\ 5 \u0026 9 \u0026 7 \\end{pmatrix} = 2 \\det(A_{11}) - 3 \\det(A_{12}) + 5 \\det(A_{13}). $$\nAlthough this definition is an efficient way to describe the determinant, it obscures many of its uses and intuitions.\nThe second way to describe the determinant incorporates its key algebraic properties. It highlights the functional properties of the determinant. Denote the ( n \\times n ) matrix ( A ) as ( A = (A_1, A_2, \\dots, A_n) ), where ( A_i ) denotes the ( i )-th column:\n:::{.definition}\nThe determinant of ( A ) is the unique real-valued function:\n$$ \\det: \\text{Matrices} \\to \\mathbb{R}, $$\nthat satisfies:\n( \\det(A_1, \\dots, c A_k, \\dots, A_n) = c \\det(A_1, \\dots, A_k, \\dots, A_n) ). ( \\det(A_1, \\dots, A_k + A_{k’}, \\dots, A_n) = \\det(A_1, \\dots, A_k, \\dots, A_n) ) for ( k \\neq k’ ). ( \\det(\\text{Identity Matrix}) = 1. )\n::: Thus, treating each column vector of a matrix as a vector in ( \\mathbb{R}^n ), the determinant can be viewed as a special function from ( \\mathbb{R}^n \\times \\dots \\times \\mathbb{R}^n ) to the real numbers.\nTo use this definition, one would need to prove that such a function on the space of matrices, satisfying these conditions, exists and is unique. Existence can be shown by verifying that our first (inductive) definition satisfies these conditions, although this involves tedious computation. The proof of uniqueness can be found in most linear algebra texts.\nThe third definition of the determinant is the most geometric but also the vaguest. Think of an ( n \\times n ) matrix ( A ) as a linear transformation from ( \\mathbb{R}^n ) to ( \\mathbb{R}^n ). Then, ( A ) maps the unit cube in ( \\mathbb{R}^n ) to some other object (a parallelepiped). The unit cube has, by definition, a volume of one.\n:::{.definition}\nThe determinant of the matrix ( A ) is the signed volume of the image of the unit cube.\n:::\nThis is not well-defined since the method of defining the volume of the image has not been described. In fact, most would define the signed volume of the image as the number given by the determinant using one of the two previous definitions. However, this can be made rigorous, albeit at the cost of losing much of the geometric intuition.\nFor example, the matrix\n$$ A = \\begin{pmatrix} 2 \u0026 0 \\ 0 \u0026 1 \\end{pmatrix} $$\nmaps the unit cube to a region with doubled area, so we must have:\n$$ \\det(A) = 2. $$\nSigned volume means that if the orientations of the edges of the unit cube are reversed, we must have a negative sign for the volume. For example, consider the matrix\n$$ A = \\begin{pmatrix} 0 \u0026 -1 \\ 1 \u0026 0 \\end{pmatrix}. $$\nHere, the image is:\n$$ \\begin{pmatrix} 1 \u0026 0 \\ 0 \u0026 -1 \\end{pmatrix}. $$\nNote that the orientations of the sides are reversed. Since the area is still doubled, the definition forces:\n$$ \\det(A) = -2. $$\nDefining orientation rigorously is somewhat complicated (we will do this in Chapter Six), but its meaning is straightforward.\nThe determinant has many algebraic properties. For example:\n:::{.lemma}\nIf ( A ) and ( B ) are ( n \\times n ) matrices, then:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B). $$\n:::\nThis can be proven via lengthy calculation or by focusing on the definition of the determinant as the change in volume of the unit cube.\nThe Key Theorem of Linear Algebra Below is the fundamental theorem of linear algebra. (Note: We have not yet defined eigenvalues or eigenvectors, but we will do so in Section 8.)\n:::{.theorem}\n(Key Theorem 1.6.1) Let ( A ) be an ( n \\times n ) matrix. Then, the following statements are equivalent:\n( A ) is invertible. ( \\det(A) \\neq 0 ). ( \\ker(A) = {0} ). If ( \\mathbf{b} ) is a column vector in ( \\mathbb{R}^n ), there exists a unique column vector ( \\mathbf{x} ) in ( \\mathbb{R}^n ) such that ( A\\mathbf{x} = \\mathbf{b} ). The columns of ( A ) are linearly independent ( n \\times 1 ) column vectors. The rows of ( A ) are linearly independent ( 1 \\times n ) row vectors. The transpose ( A^\\top ) of ( A ) is invertible. (Here, if ( A = (a_{ij}) ), then ( A^\\top = (a_{ji}) ). All eigenvalues of ( A ) are nonzero.\n::: We can restate this theorem in terms of linear transformations:\n:::{.theorem}\n(Key Theorem 1.6.2) Let ( T: V \\to V ) be a linear transformation. Then, the following statements are equivalent:\n( T ) is invertible. ( \\det(T) \\neq 0 ), where the determinant is defined via a choice of basis in ( V ). ( \\ker(T) = {0} ). If ( \\mathbf{b} ) is a vector in ( V ), there exists a unique vector ( \\mathbf{v} ) in ( V ) such that ( T(\\mathbf{v}) = \\mathbf{b} ). For any basis ( \\mathbf{v}_1, \\dots, \\mathbf{v}_n ) of ( V ), the image vectors ( T(\\mathbf{v}_1), \\dots, T(\\mathbf{v}_n) ) are linearly independent. For any basis ( \\mathbf{v}_1, \\dots, \\mathbf{v}_n ) of ( V ), if ( S ) denotes the transpose linear transformation of ( T ), then the image vectors ( S(\\mathbf{v}_1), \\dots, S(\\mathbf{v}_n) ) are linearly independent. The transpose ( T^\\top ) is invertible. (Here, the transpose is defined via a choice of basis in ( V ).) All eigenvalues of ( T ) are nonzero.\n::: To clarify the correspondence between the two theorems, we note that we currently have definitions for the determinant and the transpose only for matrices, not for linear transformations. However, both notions can be extended to linear transformations by fixing a basis (or equivalently, by choosing an inner product, which will be defined in Chapter 13 on Fourier series).\nIt is important to note that while the actual value of ( \\det(T) ) depends on the chosen basis, the condition ( \\det(T) \\neq 0 ) does not. Similar remarks apply to conditions (6) and (7).\nAn exercise (Exercise 7) encourages the reader to find any linear algebra textbook and complete the proof of this theorem. It is unlikely that the textbook will present the result in this exact form, making the act of translation itself part of the exercise’s purpose.\nEach of these equivalences is significant and can be studied in its own right. It is remarkable that they are all equivalent.\nSimilar Matrices Recall that given a coordinate system for a vector space ( V ) of dimension ( n ), we can represent a linear transformation ( T: V \\to V ) as an ( n \\times n ) matrix ( A ). However, if we choose a different coordinate system ( V’ ), the matrix representing the linear transformation ( T’ ) will generally differ from the original matrix ( A ). The goal of this section is to establish a clear criterion to determine when two matrices represent the same linear transformation under different choices of bases.\n:::{.definition}\nTwo ( n \\times n ) matrices ( A ) and ( B ) are similar if there exists an invertible matrix ( C ) such that\n$$ A = C^{-1} B C. $$\n:::\nWe aim to show that two matrices are similar precisely when they represent the same linear transformation. Let us choose two bases for the vector space ( V ), say ( {v_1, \\dots, v_n} ) (the ( v )-basis) and ( {w_1, \\dots, w_n} ) (the ( w )-basis). Let ( A ) be the matrix representing the linear transformation ( T ) with respect to the ( v )-basis, and ( B ) the matrix representing ( T ) with respect to the ( w )-basis. We seek to construct the matrix ( C ) such that\n$$ A = C^{-1} B C. $$\nRecall that given the ( v )-basis, any vector ( \\mathbf{z} \\in V ) can be written as an ( n \\times 1 ) column vector as follows: there exist unique scalars ( a_1, \\dots, a_n ) such that\n$$ \\mathbf{z} = a_1 \\mathbf{v}_1 + \\dots + a_n \\mathbf{v}_n. $$\nWe then represent ( \\mathbf{z} ) with respect to the ( v )-basis as the column vector:\n$$ \\begin{pmatrix} a_1 \\ \\vdots \\ a_n \\end{pmatrix}. $$\nSimilarly, there exist unique scalars ( b_1, \\dots, b_n ) such that\n$$ \\mathbf{z} = b_1 \\mathbf{w}_1 + \\dots + b_n \\mathbf{w}_n, $$\nwhich means that with respect to the ( w )-basis, the vector ( \\mathbf{z} ) is represented as:\n$$ \\begin{pmatrix} b_1 \\ \\vdots \\ b_n \\end{pmatrix}. $$\nThe desired matrix ( C ) is the one that satisfies the relationship:\n$$ C A = B C. $$\nDetermining when two matrices are similar is a result that frequently arises in mathematics and physics. Often, we need to select a coordinate system (a basis) to express anything, but the underlying mathematics or physics of interest is independent of the initial choice. The key question then becomes: What is preserved when the coordinate system changes? Similar matrices provide a starting point to understand these issues.\nEigenvalues and Eigenvectors In the previous section, we observed that two matrices represent the same linear transformation under different choices of bases precisely when they are similar. However, this does not tell us how to choose a basis for a vector space so that a linear transformation has a particularly simple matrix representation. For instance, the diagonal matrix\n$$ A = \\begin{pmatrix} 1 \u0026 0 \u0026 0 \\ 0 \u0026 2 \u0026 0 \\ 0 \u0026 0 \u0026 3 \\end{pmatrix} $$\nis similar to the matrix\n$$ B = \\begin{pmatrix} 1 \u0026 -5 \u0026 15 \\ 1 \u0026 -1 \u0026 84 \\ -4 \u0026 -15 \u0026 15 \\end{pmatrix}, $$\nbut the simplicity of ( A ) is evident compared to ( B ). (Incidentally, it is not obvious that ( A ) and ( B ) are similar; I started with ( A ), chose a nonsingular matrix ( C ), and then used Mathematica software to compute ( C^{-1}AC ) to obtain ( B ). This was not immediately apparent but intentionally set up.)\nOne of the purposes of the following definitions of eigenvalues and eigenvectors is to provide tools for selecting good bases. However, there are many other reasons to understand eigenvalues and eigenvectors.\n:::{.definition}\nLet ( T: V \\to V ) be a linear transformation. A nonzero vector ( \\mathbf{v} \\in V ) is an eigenvector of ( T ) with eigenvalue ( \\lambda ), a scalar, if\n$$ T(\\mathbf{v}) = \\lambda \\mathbf{v}. $$\nFor an ( n \\times n ) matrix ( A ), a nonzero column vector ( \\mathbf{x} \\in \\mathbb{R}^n ) is an eigenvector with eigenvalue ( \\lambda ), a scalar, if\n$$ A \\mathbf{x} = \\lambda \\mathbf{x}. $$\n:::\nGeometrically, a vector ( \\mathbf{v} ) is an eigenvector of the linear transformation ( T ) with eigenvalue ( \\lambda ) if ( T ) stretches ( \\mathbf{v} ) by a factor of ( \\lambda ).\nFor example, consider the matrix\n$$ \\begin{pmatrix} -2 \u0026 1 \\ 2 \u0026 7 \\end{pmatrix}. $$\nHere, ( \\lambda = 2 ) is an eigenvalue, and ( \\mathbf{v} ) is an eigenvector for the linear transformation represented by this ( 2 \\times 2 ) matrix.\nFortunately, there is a simple way to describe the eigenvalues of a square matrix, allowing us to see that the eigenvalues of a matrix are preserved under a similarity transformation.\n:::{.definition}\nA scalar ( \\lambda ) is an eigenvalue of a square matrix ( A ) if and only if ( \\lambda ) is a root of the polynomial\n$$ P(t) = \\det(tI - A), $$\nwhere ( P(t) ) is called the characteristic polynomial of ( A ).\n:::\nProof: Suppose ( \\lambda ) is an eigenvalue of ( A ), with eigenvector ( \\mathbf{v} ). Then,\n$$ A \\mathbf{v} = \\lambda \\mathbf{v}, $$\nwhich implies\n$$ A \\mathbf{v} - \\lambda \\mathbf{v} = 0. $$\nIntroducing the identity matrix ( I ), we rewrite this as\n$$ 0 = (\\lambda I - A) \\mathbf{v}. $$\nThus, the matrix ( \\lambda I - A ) has a nontrivial kernel, ( \\mathbf{v} ). By the key theorem of linear algebra, this occurs precisely when\n$$ \\det(\\lambda I - A) = 0, $$\nwhich means ( \\lambda ) is a root of the characteristic polynomial ( P(t) = \\det(tI - A) ). Since these implications are reversible, the theorem is established.\n:::{.theorem}\nIf ( A ) and ( B ) are similar matrices, then the characteristic polynomial of ( A ) is equal to the characteristic polynomial of ( B ).\n:::\nProof: For ( A ) and ( B ) to be similar, there must exist an invertible matrix ( C ) such that ( A = C^{-1}BC ). Then,\n$$ \\det(tI - A) = \\det(tI - C^{-1}BC) = \\det(C^{-1}(tI - B)C) = \\det(C^{-1})\\det(tI - B)\\det(C) = \\det(tI - B), $$\nusing the property ( \\det(C^{-1}C) = 1 = \\det(C^{-1})\\det(C) ).\nSince similar matrices have the same characteristic polynomial, it follows that their eigenvalues must also be the same.\n:::{.corollary}\nThe eigenvalues of similar matrices are identical. Thus, to determine if two matrices are not similar, check whether their eigenvalues differ. If they do, the matrices are not similar.\nHowever, in general, identical eigenvalues do not imply that matrices are similar. For example, the matrices\n$$ A = \\begin{pmatrix} 4 \u0026 2 \\ 7 \u0026 5 \\end{pmatrix} \\quad \\text{and} \\quad B = \\begin{pmatrix} 2 \u0026 3 \\ 5 \u0026 4 \\end{pmatrix} $$\nhave eigenvalues ( 1 ) and ( 2 ), but they are not similar. (This can be shown by assuming the existence of an invertible matrix ( C ) such that ( C^{-1}AC = B ) and then demonstrating that ( \\det(C) = 0 ), which contradicts the invertibility of ( C ).)\n:::\nEigenvalues and Determinants Since the characteristic polynomial ( P(t) ) does not change under a similarity transformation, the coefficients of ( P(t) ) are also preserved. As these coefficients are polynomial functions (complex) of the entries of the matrix ( A ), we obtain special polynomials of the entries of ( A ) that are invariant under similarity transformations. One such coefficient, already seen in another form, is the determinant of ( A ), as highlighted in the following theorem. This theorem establishes a deeper connection between the eigenvalues of ( A ) and the determinant of ( A ).\n:::{.theorem}\nLet ( \\lambda_1, \\lambda_2, \\dots, \\lambda_n ) be the eigenvalues (counted with multiplicity) of a matrix ( A ). Then,\n$$ \\det(A) = \\lambda_1 \\cdot \\lambda_2 \\cdot \\dots \\cdot \\lambda_n. $$\n:::\nBefore proving this theorem, we need to clarify the concept of counting eigenvalues “with multiplicity.” The difficulty arises because a polynomial can have a root that must be counted more than once. For example, the polynomial ( (z - 2)^2 ) has a single root ( 2 ), but we count it twice. This situation is particularly relevant for the characteristic polynomial.\nFor instance, consider the matrix\n$$ A = \\begin{pmatrix} 5 \u0026 0 \u0026 0 \\ 0 \u0026 5 \u0026 0 \\ 0 \u0026 0 \u0026 4 \\end{pmatrix}, $$\nwhich has the characteristic polynomial ( P(t) = (t - 5)(t - 5)(t - 4) ). Here, we list the eigenvalues as ( 4, 5, ) and ( 5 ), counting the eigenvalue ( 5 ) twice.\nProof: Since the eigenvalues ( \\lambda_1, \\lambda_2, \\dots, \\lambda_n ) are the roots of the characteristic polynomial ( \\det(tI - A) ), we can write\n$$ \\det(tI - A) = (t - \\lambda_1)(t - \\lambda_2) \\cdots (t - \\lambda_n). $$\nSetting ( t = 0 ), we obtain\n$$ (-1)^n \\lambda_1 \\lambda_2 \\cdots \\lambda_n = \\det(-A). $$\nIn the matrix ( -A ), each column of ( A ) is multiplied by ( -1 ). Using the linearity of the determinant, we can factor out ( -1 ) from each column, yielding\n$$ \\det(-A) = (-1)^n \\det(A). $$\nSubstituting, we have\n$$ (-1)^n \\lambda_1 \\lambda_2 \\cdots \\lambda_n = (-1)^n \\det(A), $$\nand thus,\n$$ \\det(A) = \\lambda_1 \\cdot \\lambda_2 \\cdots \\lambda_n. $$\nTo find a “good” basis for representing a linear transformation, we measure “goodness” by how close the matrix is to being diagonal. Here, we focus on a special but common class of matrices: symmetric matrices. A matrix ( A ) is symmetric if ( A = (a_{ij}) ) satisfies ( a_{ij} = a_{ji} ), meaning the element in the ( i )-th row and ( j )-th column equals the element in the ( j )-th row and ( i )-th column.\nFor example:\n$$ \\begin{pmatrix} 5 \u0026 3 \u0026 4 \\ 3 \u0026 5 \u0026 2 \\ 4 \u0026 2 \u0026 4 \\end{pmatrix} $$\nis symmetric, but\n$$ \\begin{pmatrix} 5 \u0026 6 \u0026 2 \\ 2 \u0026 5 \u0026 18 \\ 3 \u0026 3 \u0026 4 \\end{pmatrix} $$\nis not.\n:::{.theorem}\nIf ( A ) is a symmetric matrix, then there exists a matrix ( B ) similar to ( A ) that is diagonal. Moreover, the entries along the diagonal of ( B ) are precisely the eigenvalues of ( A ).\n:::\nProof: The proof relies on showing that the eigenvectors of ( A ) form a basis such that ( A ) becomes diagonal in this basis. We assume that ( A )’s eigenvalues are distinct since technical difficulties arise when eigenvalues have multiplicity.\nLet ( v_1, v_2, \\dots, v_n ) be the eigenvectors of ( A ) with corresponding eigenvalues ( \\lambda_1, \\lambda_2, \\dots, \\lambda_n ). Form the matrix\n$$ C = \\begin{pmatrix} v_1 \u0026 v_2 \u0026 \\dots \u0026 v_n \\end{pmatrix}, $$\nwhere the ( i )-th column of ( C ) is the column vector ( v_i ). We will show that the matrix ( C^{-1}AC ) satisfies our theorem. Specifically, we aim to demonstrate\n$$ C^{-1}AC = B, $$\nwhere ( B ) is the desired diagonal matrix.\nDefine ( B ) as\n$$ B = \\begin{pmatrix} \\lambda_1 \u0026 0 \u0026 \\cdots \u0026 0 \\ 0 \u0026 \\lambda_2 \u0026 \\cdots \u0026 0 \\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\ 0 \u0026 0 \u0026 \\cdots \u0026 \\lambda_n \\end{pmatrix}. $$\nThe diagonal matrix ( B ) is the only matrix satisfying ( Be_i = \\lambda_i e_i ) for all ( i ), where ( e_i ) is the standard basis vector. Observe that ( Ce_i = v_i ) for all ( i ). Thus,\n$$ C^{-1}ACe_i = C^{-1}Av_i = C^{-1}(\\lambda_i v_i) = \\lambda_i C^{-1}v_i = \\lambda_i e_i, $$\nproving that\n$$ C^{-1}AC = B. $$\nThis result is not the end of the story. For nonsymmetric matrices, there are other canonical forms to find “good” similar matrices, such as the Jordan canonical form, upper triangular form, and rational canonical form.\n","wordCount":"6328","inLanguage":"en","image":"https://jadrk040507.github.io/skope/methodology/public/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-12-11T00:00:00Z","dateModified":"2024-12-11T00:00:00Z","author":{"@type":"Person","name":"Juan Alvaro Diaz Raimond Kedilhac"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jadrk040507.github.io/skope/methodology/public/post/2024-12-11-linear-algebra/"},"publisher":{"@type":"Organization","name":"ExampleSite","logo":{"@type":"ImageObject","url":"https://jadrk040507.github.io/skope/methodology/public/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jadrk040507.github.io/skope/methodology/public/ accesskey=h title="Home (Alt + H)"><img src=https://jadrk040507.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jadrk040507.github.io/skope/methodology/public/about/ title=About><span>About</span></a></li><li><a href=https://jadrk040507.github.io/skope/methodology/public/categories/ title=categories><span>categories</span></a></li><li><a href=https://jadrk040507.github.io/skope/methodology/public/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jadrk040507.github.io/skope/methodology/public/>Home</a>&nbsp;»&nbsp;<a href=https://jadrk040507.github.io/skope/methodology/public/post/>Posts</a></div><h1 class="post-title entry-hint-parent">Linear Algebra</h1><div class=post-meta><span title='2024-12-11 00:00:00 +0000 UTC'>December 11, 2024</span>&nbsp;·&nbsp;30 min&nbsp;·&nbsp;6328 words&nbsp;·&nbsp;Juan Alvaro Diaz Raimond Kedilhac&nbsp;|&nbsp;<a href=https://github.com/jadrk040507/skope/tree/main/methodology/content/post/2024-12-11-linear-algebra/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>Based on @garrity_2007</p><h2 id=the-basic-vector-space---mathbbrn->The Basic Vector Space ( = \mathbb{R}^n )<a hidden class=anchor aria-hidden=true href=#the-basic-vector-space---mathbbrn->#</a></h2><p>The quintessential vector space is (R^n ), which is defined as the set of all ( n )-tuples of real numbers. As we will see in the next section, what makes this set a vector space is the ability to add two ( n )-tuples to obtain another ( n )-tuple:</p><p>$$
(v_1, \ldots, v_n) + (w_1, \ldots, w_n) = (v_1 + w_1, \ldots, v_n + w_n)
$$</p><p>and to multiply each ( n )-tuple by a real number ( \lambda ):</p><p>$$
\lambda (v_1, \ldots, v_n) = (\lambda v_1, \ldots, \lambda v_n)
$$</p><p>In this way, each ( n )-tuple is commonly referred to as a vector, and the real numbers ( \lambda ) are known as scalars. When ( n = 2 ) or ( n = 3 ), this reduces to vectors in the plane and in space, which most of us learned in high school.</p><p>The natural relationship from ( \mathbb{R}^n ) to ( \mathbb{R}^m ) is established through matrix multiplication. We write a vector ( \mathrm{x} \in \mathbb{R}^n ) as a column vector:</p><p>$$
\mathrm{x} = \begin{pmatrix} x_1 \ x_2 \ \vdots \ x_n \end{pmatrix}
$$</p><p>Similarly, we can write a vector in ( \mathbb{R}^m ) as a column vector with ( m ) entries. Let ( A ) be an ( m \times n ) matrix:</p><p>$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \
a_{21} & a_{22} & \cdots & a_{2n} \
\vdots & \vdots & \ddots & \vdots \
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
$$</p><p>Then, the product ( A \mathrm{x} ) is the ( m )-tuple:</p><p>$$
A \mathrm{x} = \begin{pmatrix}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \
\vdots \
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n
\end{pmatrix}
$$</p><p>For any two vectors ( \mathrm{x} ) and ( \mathrm{y} ) in ( \mathbb{R}^n ) and any two scalars ( \lambda ) and ( \mu ), the following property holds:</p><p>$$
A (\lambda \mathrm{x} + \mu \mathrm{y}) = \lambda A \mathrm{x} + \mu A \mathrm{y}
$$</p><p>In the next section, we will use the linearity of matrix multiplication to motivate the definition of a linear transformation between vector spaces. Now, let&rsquo;s relate all this to solving a system of linear equations. Suppose we are given numbers ( b_1, \ldots, b_m ) and numbers ( a_{ij}, \ldots, a_{mn} ). Our goal is to find ( n ) numbers ( x_1, \ldots, x_n ) that solve the following system of linear equations:</p><p>$$
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2 \
&\vdots \
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m
\end{aligned}
$$</p><p>Calculations in linear algebra often reduce to solving a system of linear equations. When there are only a few equations, we can find the solutions manually, but as the number of equations increases, the calculations quickly become less about pleasant algebraic manipulations and more about keeping track of many small individual details. In other words, it is an organizational problem.</p><p>We can write:</p><p>$$
\mathrm{b} = \begin{pmatrix} b_1 \ b_2 \ \vdots \ b_m \end{pmatrix}
$$</p><p>and our unknowns as:</p><p>$$
\mathrm{x} = \begin{pmatrix} x_1 \ x_2 \ \vdots \ x_n \end{pmatrix}
$$</p><p>Then, we can rewrite our system of linear equations in the more visually appealing form:</p><p>$$
A \mathrm{x} = \mathrm{b}
$$</p><p>When ( m > n ) (when there are more equations than unknowns), we generally do not expect solutions. For example, when ( m = 3 ) and ( n = 2 ), this corresponds geometrically to the fact that three lines in a plane generally do not have a common intersection point. When ( m &lt; n ) (when there are more unknowns than equations), we generally expect there to be many solutions. In the case where ( m = 2 ) and ( n = 3 ), this corresponds geometrically to the fact that two planes in space generally intersect in an entire line. Much of the machinery of linear algebra deals with the remaining case when ( m = n ).</p><p>Therefore, we want to find the ( n \times 1 ) column vector ( \mathrm{x} ) that solves ( A \mathrm{x} = \mathrm{b} ), where ( A ) is a given ( n \times n ) matrix and ( \mathrm{b} ) is a given ( n \times 1 ) column vector.</p><p>Suppose the square matrix ( A ) has an inverse matrix ( A^{-1} ) (which means that ( A^{-1} ) is also ( n \times n ) and, more importantly, that ( A A^{-1} = I ), where ( I ) is the identity matrix). Then our solution will be:</p><p>$$
\mathrm{x} = A^{-1} \mathrm{b}
$$</p><p>because:</p><p>$$
A \mathrm{x} = A (A^{-1} \mathrm{b}) = I \mathrm{b} = \mathrm{b}
$$</p><p>Thus, solving our system of linear equations reduces to understanding when the ( n \times n ) matrix ( A ) has an inverse. (If an inverse matrix exists, then algorithms exist for its calculation). The key theorem of linear algebra, which is stated in section six, is essentially a list of many equivalencies for when an ( n \times n ) matrix has an inverse, and it is therefore crucial to understanding when a system of linear equations can be solved.</p><h2 id=vector-spaces-and-linear-transformations>Vector Spaces and Linear Transformations<a hidden class=anchor aria-hidden=true href=#vector-spaces-and-linear-transformations>#</a></h2><p>The abstract approach to studying systems of linear equations begins with the concept of a vector space.</p><p>:::{.definition}<br>A set ( V ) is a <strong>vector space</strong> over the real numbers ( \mathbb{R} ) if there are two operations:</p><ol><li><p><strong>Scalar multiplication</strong>: For every ( a \in \mathbb{R} ) and ( \mathrm{v} \in V ), there exists an element ( a \cdot \mathrm{v} \in V ), denoted ( a\mathrm{v} ), satisfying the properties of scalar multiplication.</p></li><li><p><strong>Vector addition</strong>: For every ( \mathrm{v}, \mathrm{w} \in V ), there exists an element ( \mathrm{v} + \mathrm{w} \in V ), denoted ( \mathrm{v} + \mathrm{w} ), satisfying the properties of vector addition.</p></li></ol><p>These operations must satisfy the following properties:</p><ul><li><strong>Additive identity</strong>: There exists an element ( \mathrm{0} \in V ) (the zero vector) such that ( \mathrm{0} + \mathrm{v} = \mathrm{v} ) for all ( \mathrm{v} \in V ).</li><li><strong>Additive inverse</strong>: For every ( \mathrm{v} \in V ), there exists an element ( -\mathrm{v} \in V ) such that ( \mathrm{v} + (-\mathrm{v}) = \mathrm{0} ).</li><li><strong>Commutativity</strong>: For all ( \mathrm{v}, \mathrm{w} \in V ), ( \mathrm{v} + \mathrm{w} = \mathrm{w} + \mathrm{v} ).</li><li><strong>Distributivity of scalar multiplication over vector addition</strong>: For all ( a \in \mathbb{R} ) and ( \mathrm{v}, \mathrm{w} \in V ), ( a(\mathrm{v} + \mathrm{w}) = a\mathrm{v} + a\mathrm{w} ).</li><li><strong>Distributivity of scalar multiplication over scalar addition</strong>: For all ( a, b \in \mathbb{R} ) and ( \mathrm{v} \in V ), ( (a + b)\mathrm{v} = a\mathrm{v} + b\mathrm{v} ).</li><li><strong>Compatibility of scalar multiplication with field multiplication</strong>: For all ( a, b \in \mathbb{R} ) and ( \mathrm{v} \in V ), ( a(b\mathrm{v}) = (ab)\mathrm{v} ).</li><li><strong>Multiplicative identity</strong>: For all ( \mathrm{v} \in V ), ( 1 \cdot \mathrm{v} = \mathrm{v} ), where 1 is the multiplicative identity in ( \mathbb{R} ).<br>:::</li></ul><p>The field of scalars can be replaced by other fields, such as ( \mathrm{C} ), without changing the general concept of vector spaces.</p><p>Elements of a vector space are referred to as <strong>vectors</strong>, while elements of the scalar field (e.g., ( \mathbb{R} )) are called <strong>scalars</strong>. For example, ( \mathbb{R}^n ), the space of all ( n )-tuples of real numbers, is a vector space that satisfies these axioms.</p><p>The natural mappings between vector spaces are linear transformations.</p><p>:::{.definition}<br>A <strong>linear transformation</strong> ( T : V \to W ) is a function from a vector space ( V ) to a vector space ( W ) such that for all ( a_1, a_2 \in \mathbb{R} ) and ( \mathrm{v}_1, \mathrm{v}_2 \in V ):<br>$$
T(a_1 \mathrm{v}_1 + a_2 \mathrm{v}_2) = a_1 T(\mathrm{v}_1) + a_2 T(\mathrm{v}_2).
$$</p><p>An example of a linear transformation is matrix multiplication, mapping ( \mathbb{R}^n ) to ( \mathbb{R}^m ).<br>:::</p><p>:::{.definition}<br>A subset ( U ) of a vector space ( V ) is called a <strong>subspace</strong> if ( U ) itself is a vector space under the operations of ( V ).</p><p>To determine whether a subset is a subspace, we use the following proposition:<br>:::</p><p>:::{.proposition}<br>A subset ( U ) of a vector space ( V ) is a subspace if it is closed under addition and scalar multiplication.<br>:::</p><p>Given a linear transformation ( T : V \to W ), we can naturally define two important subspaces of ( V ) and ( W ).</p><p>:::{.definition}<br>If ( T : V \to W ) is a linear transformation, then:</p><ul><li>The <strong>kernel</strong> of ( T ) is<br>$$
\ker(T) = { \mathrm{v} \in V : T(\mathrm{v}) = 0 }.
$$</li><li>The <strong>image</strong> of ( T ) is<br>$$
\text{Im}(T) = { \mathrm{w} \in W : \text{there exists } \mathrm{v} \in V \text{ such that } T(\mathrm{v}) = \mathrm{w} }.
$$</li></ul><p>The kernel of ( T ) is a subspace of ( V ), as closure under addition and scalar multiplication can be verified. Similarly, the image of ( T ) is a subspace of ( W ).<br>:::</p><h3 id=an-example-beyond-finite-dimensions>An Example Beyond Finite Dimensions<a hidden class=anchor aria-hidden=true href=#an-example-beyond-finite-dimensions>#</a></h3><p>If all vector spaces were finite-dimensional (like ( \mathbb{R}^n )), the above abstraction would be trivial. However, vector spaces can also include function spaces.</p><p>Consider the set ( C^<em>[0,1] ) of all real-valued functions defined on ( [0,1] ) such that the ( k )-th derivative exists and is continuous. The sum of two such functions and the scalar multiplication of a function by a real number remain in ( C^</em>[0,1] ), making it a vector space. Unlike ( \mathbb{R}^n ), ( C^*[0,1] ) is infinite-dimensional.</p><p>The derivative operator defines a linear transformation:<br>$$
\frac{d}{dx} : C^*[0,1] \to C^{k-1}[0,1].
$$</p><p>The kernel of this transformation is the set of functions whose ( k )-th derivative is zero, i.e., the set of constant functions.</p><p>Now consider the second-order differential equation:<br>$$
f&rsquo;&rsquo; + 3f&rsquo; + 2f = 0.
$$</p><p>Let ( T ) be the linear transformation defined by:<br>$$
T(f) = f&rsquo;&rsquo; + 3f&rsquo; + 2f.
$$</p><p>Finding a solution ( f(x) ) to the differential equation corresponds to finding an element in ( \ker(T) ). This illustrates how the language of linear algebra provides tools for studying (linear) differential equations.</p><h2 id=bases-dimension-and-linear-transformations-as-matrices>Bases, Dimension, and Linear Transformations as Matrices<a hidden class=anchor aria-hidden=true href=#bases-dimension-and-linear-transformations-as-matrices>#</a></h2><p>Our next goal is to define the dimension of a vector space.</p><p>:::{.definition}
A set of vectors ((v_1, \dots, v_n)) forms a basis for the vector space (V) if, for any vector (v \in V), there exist unique scalars (a_1, \dots, a_n \in \mathbb{R}) such that</p><p>[
v = a_1v_1 + \dots + a_nv_n.
]
:::</p><p>:::{.definition}
The dimension of a vector space (V), denoted as (\text{dim}(V)), is the number of elements in a basis.
:::</p><p>It is not obvious that the number of elements in a basis will always be the same, regardless of the chosen basis. To ensure that the definition of the dimension of a vector space is well-defined, we need the following theorem (which we will not prove):</p><p>:::{.definition}
All bases of a vector space (V) have the same number of elements.
:::</p><p>For (\mathbb{R}^n), the usual basis is ({(1, 0, \dots, 0), (0, 1, 0, \dots, 0), \dots, (0, \dots, 0, 1)}). Therefore, (\mathbb{R}^n) has dimension (n). If this were not the case, the previous definition of dimension would be incorrect and we would need another. This is an example of the principle mentioned in the introduction. We have an intuitive understanding of what the dimension should mean for specific examples: a line should be one-dimensional, a plane two-dimensional, and three-dimensional space. We then formulate a precise definition. If this definition gives the &ldquo;correct answer&rdquo; for our three already understood examples, we are somewhat confident that the definition has captured what dimension means in this case. We can then apply the definition to examples where our intuitions fail.</p><p>Linked to the idea of a basis is:</p><p>:::{.definition}
The vectors ((v_1, \dots, v_n)) in a vector space (V) are linearly independent if, whenever (a_1v_1 + \dots + a_nv_n = 0), it must be the case that the scalars (a_1, \dots, a_n) are all zero. Intuitively, a set of vectors is linearly independent if they all point in different directions. A basis, therefore, consists of a set of linearly independent vectors that span the vector space, where &ldquo;span&rdquo; means:
:::</p><p>:::{.definition}
A set of vectors ((v_1, \dots, v_n)) spans the vector space (V) if, for any vector (v \in V), there exist scalars (a_1, \dots, a_n \in \mathbb{R}) such that</p><p>[
v = a_1v_1 + \dots + a_nv_n.
]
:::</p><p>Our next goal is to show how all linear transformations (T: V \to W) between finite-dimensional spaces can be represented as matrix multiplication, provided that we fix bases for the vector spaces (V) and (W).</p><p>First, we fix a basis ({v_1, \dots, v_n}) for (V) and a basis ({w_1, \dots, w_m}) for (W). Before examining the linear transformation (T), we need to show how each element of the (n)-dimensional space (V) can be represented as a column vector in (\mathbb{R}^n) and how each element of the (m)-dimensional space (W) can be represented as a column vector in (\mathbb{R}^m). Given any vector (v \in V), by the definition of a basis, there exist unique real numbers (a_1, \dots, a_n) such that:</p><p>[
v = a_1v_1 + \dots + a_nv_n.
]</p><p>Thus, we represent the vector (v) with the column vector:</p><p>[
\begin{pmatrix} a_1 \ a_2 \ \vdots \ a_n \end{pmatrix}.
]</p><p>Similarly, for any vector (w \in W), there exist unique real numbers (b_1, \dots, b_m) such that:</p><p>[
w = b_1w_1 + \dots + b_mw_m.
]</p><p>Here, we represent (w) as the column vector:</p><p>[
\begin{pmatrix} b_1 \ b_2 \ \vdots \ b_m \end{pmatrix}.
]</p><p>It is important to note that we have established a correspondence between the vectors in (V) and (W) and the column vectors in (\mathbb{R}^n) and (\mathbb{R}^m), respectively. More technically, we can prove that (V) is isomorphic to (\mathbb{R}^n) (which means there is a one-to-one and onto linear transformation from (V) to (\mathbb{R}^n)) and that (W) is isomorphic to (\mathbb{R}^m), although it must be emphasized that the real correspondence only exists after a basis has been chosen (which means that, while the isomorphism exists, it is not canonical; this is an important aspect because, in practice, we are often not provided with a basis).</p><p>Now, we want to represent a linear transformation (T: V \to W) as a matrix (A) of size (m \times n). For each basis vector (v_i) in the vector space (V), (T(v_i)) will be a vector in (W). Therefore, there will be real numbers (a_{ij}, \dots, a_{im}) such that:</p><p>[
T(v_i) = a_{ij}w_1 + \dots + a_{im}w_m.
]</p><p>We want to see that the linear transformation (T) corresponds to the matrix (A):</p><p>[
A = \begin{pmatrix} a_{11} & a_{21} & \dots & a_{m1} \ a_{12} & a_{22} & \dots & a_{m2} \ \vdots & \vdots & \ddots & \vdots \ a_{1n} & a_{2n} & \dots & a_{mn} \end{pmatrix}.
]</p><p>Given any vector (v \in V), with (v = a_1v_1 + \dots + a_nv_n), we have:</p><p>[
T(v) = a_1T(v_1) + \dots + a_nT(v_n) = a_1(a_{11}w_1 + \dots + a_{1m}w_m) + \dots + a_n(a_{n1}w_1 + \dots + a_{nm}w_m).
]</p><p>Under the correspondences of the vector spaces with the respective column spaces, this can be seen as the matrix multiplication of (A) by the column vector corresponding to the vector (v):</p><p>[
A \begin{pmatrix} a_1 \ a_2 \ \vdots \ a_n \end{pmatrix} = \begin{pmatrix} b_1 \ b_2 \ \vdots \ b_m \end{pmatrix}.
]</p><p>It is important to note that if (T: V \to V) is a linear transformation of a vector space onto itself, then the corresponding matrix will be (n \times n), i.e., a square matrix.</p><p>Given different bases for the vector spaces (V) and (W), the matrix associated with the linear transformation (T) will change. A natural problem is to determine when two matrices actually represent the same linear transformation, but under different bases. This will be the subject of section seven.</p><h2 id=the-determinant>The Determinant<a hidden class=anchor aria-hidden=true href=#the-determinant>#</a></h2><p>Our next task is to define the determinant of a matrix. In fact, we will present three alternative descriptions of the determinant. These descriptions are equivalent, and each has its own advantages.</p><p>The first method involves defining the determinant of a ( 1 \times 1 ) matrix and then recursively defining the determinant of an ( n \times n ) matrix. Since ( 1 \times 1 ) matrices are simply numbers, the following should not be surprising:</p><p>:::{.definition}<br>The determinant of a ( 1 \times 1 ) matrix ( a ) is the real-valued function:<br>$$ \det(a) = a. $$<br>:::</p><p>This should not yet seem significant.</p><p>Before defining the determinant for a general ( n \times n ) matrix, we need some notation. For an ( n \times n ) matrix:<br>$$ A =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \
a_{21} & a_{22} & \cdots & a_{2n} \
\vdots & \vdots & \ddots & \vdots \
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix},
$$<br>we denote by ( A_{ij} ) the ( (n - 1) \times (n - 1) ) matrix obtained from ( A ) by removing the ( i )-th row and ( j )-th column. For example, if<br>$$ A =
\begin{pmatrix}
2 & 6 & 1 \
3 & 4 & 8 \
5 & 9 & 7
\end{pmatrix},
$$<br>then<br>$$ A_{23} =
\begin{pmatrix}
2 & 1 \
3 & 8
\end{pmatrix}.
$$<br>Similarly, if<br>$$ A =
\begin{pmatrix}
6 & 7 \
4 & 1 \
9 & 8
\end{pmatrix},
$$<br>then<br>$$ A_{12} =
\begin{pmatrix}
7 & 8
\end{pmatrix}.
$$</p><p>Since we have a definition for the determinant of ( 1 \times 1 ) matrices, we will assume by induction that we know the determinant of any ( (n - 1) \times (n - 1) ) matrix and use this to find the determinant of an ( n \times n ) matrix.</p><p>:::{.definition}<br>Let ( A ) be an ( n \times n ) matrix. The determinant of ( A ) is defined as:<br>$$ \det(A) = \sum_{k=1}^n a_{1k} \det(A_{1k}). $$<br>:::</p><p>Thus, for<br>$$ A =
\begin{pmatrix}
a_{11} & a_{12} \
a_{21} & a_{22}
\end{pmatrix},
$$<br>we have:<br>$$ \det(A) = a_{11} \det(A_{11}) - a_{12} \det(A_{12}), $$<br>which aligns with the determinant formula most of us think of. The determinant of our earlier ( 3 \times 3 ) matrix is:<br>$$ \det
\begin{pmatrix}
2 & 6 & 1 \
3 & 4 & 8 \
5 & 9 & 7
\end{pmatrix}
= 2 \det(A_{11}) - 3 \det(A_{12}) + 5 \det(A_{13}). $$</p><p>Although this definition is an efficient way to describe the determinant, it obscures many of its uses and intuitions.</p><p>The second way to describe the determinant incorporates its key algebraic properties. It highlights the functional properties of the determinant. Denote the ( n \times n ) matrix ( A ) as ( A = (A_1, A_2, \dots, A_n) ), where ( A_i ) denotes the ( i )-th column:</p><p>:::{.definition}<br>The determinant of ( A ) is the unique real-valued function:<br>$$ \det: \text{Matrices} \to \mathbb{R}, $$<br>that satisfies:</p><ol><li>( \det(A_1, \dots, c A_k, \dots, A_n) = c \det(A_1, \dots, A_k, \dots, A_n) ).</li><li>( \det(A_1, \dots, A_k + A_{k&rsquo;}, \dots, A_n) = \det(A_1, \dots, A_k, \dots, A_n) ) for ( k \neq k&rsquo; ).</li><li>( \det(\text{Identity Matrix}) = 1. )<br>:::</li></ol><p>Thus, treating each column vector of a matrix as a vector in ( \mathbb{R}^n ), the determinant can be viewed as a special function from ( \mathbb{R}^n \times \dots \times \mathbb{R}^n ) to the real numbers.</p><p>To use this definition, one would need to prove that such a function on the space of matrices, satisfying these conditions, exists and is unique. Existence can be shown by verifying that our first (inductive) definition satisfies these conditions, although this involves tedious computation. The proof of uniqueness can be found in most linear algebra texts.</p><p>The third definition of the determinant is the most geometric but also the vaguest. Think of an ( n \times n ) matrix ( A ) as a linear transformation from ( \mathbb{R}^n ) to ( \mathbb{R}^n ). Then, ( A ) maps the unit cube in ( \mathbb{R}^n ) to some other object (a parallelepiped). The unit cube has, by definition, a volume of one.</p><p>:::{.definition}<br>The determinant of the matrix ( A ) is the signed volume of the image of the unit cube.<br>:::</p><p>This is not well-defined since the method of defining the volume of the image has not been described. In fact, most would define the signed volume of the image as the number given by the determinant using one of the two previous definitions. However, this can be made rigorous, albeit at the cost of losing much of the geometric intuition.</p><p>For example, the matrix<br>$$ A =
\begin{pmatrix}
2 & 0 \
0 & 1
\end{pmatrix}
$$<br>maps the unit cube to a region with doubled area, so we must have:<br>$$ \det(A) = 2. $$</p><p>Signed volume means that if the orientations of the edges of the unit cube are reversed, we must have a negative sign for the volume. For example, consider the matrix<br>$$ A =
\begin{pmatrix}
0 & -1 \
1 & 0
\end{pmatrix}. $$<br>Here, the image is:<br>$$ \begin{pmatrix}
1 & 0 \
0 & -1
\end{pmatrix}. $$<br>Note that the orientations of the sides are reversed. Since the area is still doubled, the definition forces:<br>$$ \det(A) = -2. $$</p><p>Defining orientation rigorously is somewhat complicated (we will do this in Chapter Six), but its meaning is straightforward.</p><p>The determinant has many algebraic properties. For example:</p><p>:::{.lemma}<br>If ( A ) and ( B ) are ( n \times n ) matrices, then:<br>$$ \det(AB) = \det(A) \cdot \det(B). $$<br>:::</p><p>This can be proven via lengthy calculation or by focusing on the definition of the determinant as the change in volume of the unit cube.</p><h2 id=the-key-theorem-of-linear-algebra>The Key Theorem of Linear Algebra<a hidden class=anchor aria-hidden=true href=#the-key-theorem-of-linear-algebra>#</a></h2><p>Below is the fundamental theorem of linear algebra. <em>(Note: We have not yet defined eigenvalues or eigenvectors, but we will do so in Section 8.)</em></p><p>:::{.theorem}<br>(Key Theorem 1.6.1) Let ( A ) be an ( n \times n ) matrix. Then, the following statements are equivalent:</p><ol><li>( A ) is invertible.</li><li>( \det(A) \neq 0 ).</li><li>( \ker(A) = {0} ).</li><li>If ( \mathbf{b} ) is a column vector in ( \mathbb{R}^n ), there exists a unique column vector ( \mathbf{x} ) in ( \mathbb{R}^n ) such that ( A\mathbf{x} = \mathbf{b} ).</li><li>The columns of ( A ) are linearly independent ( n \times 1 ) column vectors.</li><li>The rows of ( A ) are linearly independent ( 1 \times n ) row vectors.</li><li>The transpose ( A^\top ) of ( A ) is invertible. <em>(Here, if ( A = (a_{ij}) ), then ( A^\top = (a_{ji}) ).</em></li><li>All eigenvalues of ( A ) are nonzero.<br>:::</li></ol><p>We can restate this theorem in terms of linear transformations:</p><p>:::{.theorem}<br>(Key Theorem 1.6.2) Let ( T: V \to V ) be a linear transformation. Then, the following statements are equivalent:</p><ol><li>( T ) is invertible.</li><li>( \det(T) \neq 0 ), where the determinant is defined via a choice of basis in ( V ).</li><li>( \ker(T) = {0} ).</li><li>If ( \mathbf{b} ) is a vector in ( V ), there exists a unique vector ( \mathbf{v} ) in ( V ) such that ( T(\mathbf{v}) = \mathbf{b} ).</li><li>For any basis ( \mathbf{v}_1, \dots, \mathbf{v}_n ) of ( V ), the image vectors ( T(\mathbf{v}_1), \dots, T(\mathbf{v}_n) ) are linearly independent.</li><li>For any basis ( \mathbf{v}_1, \dots, \mathbf{v}_n ) of ( V ), if ( S ) denotes the transpose linear transformation of ( T ), then the image vectors ( S(\mathbf{v}_1), \dots, S(\mathbf{v}_n) ) are linearly independent.</li><li>The transpose ( T^\top ) is invertible. <em>(Here, the transpose is defined via a choice of basis in ( V ).)</em></li><li>All eigenvalues of ( T ) are nonzero.<br>:::</li></ol><p>To clarify the correspondence between the two theorems, we note that we currently have definitions for the determinant and the transpose only for matrices, not for linear transformations. However, both notions can be extended to linear transformations by fixing a basis (or equivalently, by choosing an inner product, which will be defined in Chapter 13 on Fourier series).</p><p>It is important to note that while the actual value of ( \det(T) ) depends on the chosen basis, the condition ( \det(T) \neq 0 ) does not. Similar remarks apply to conditions (6) and (7).</p><p>An exercise (Exercise 7) encourages the reader to find any linear algebra textbook and complete the proof of this theorem. It is unlikely that the textbook will present the result in this exact form, making the act of translation itself part of the exercise&rsquo;s purpose.</p><p>Each of these equivalences is significant and can be studied in its own right. It is remarkable that they are all equivalent.</p><h2 id=similar-matrices>Similar Matrices<a hidden class=anchor aria-hidden=true href=#similar-matrices>#</a></h2><p>Recall that given a coordinate system for a vector space ( V ) of dimension ( n ), we can represent a linear transformation ( T: V \to V ) as an ( n \times n ) matrix ( A ). However, if we choose a different coordinate system ( V&rsquo; ), the matrix representing the linear transformation ( T&rsquo; ) will generally differ from the original matrix ( A ). The goal of this section is to establish a clear criterion to determine when two matrices represent the same linear transformation under different choices of bases.</p><p>:::{.definition}<br>Two ( n \times n ) matrices ( A ) and ( B ) are <strong>similar</strong> if there exists an invertible matrix ( C ) such that<br>$$
A = C^{-1} B C.
$$<br>:::</p><p>We aim to show that two matrices are similar precisely when they represent the same linear transformation. Let us choose two bases for the vector space ( V ), say ( {v_1, \dots, v_n} ) (the ( v )-basis) and ( {w_1, \dots, w_n} ) (the ( w )-basis). Let ( A ) be the matrix representing the linear transformation ( T ) with respect to the ( v )-basis, and ( B ) the matrix representing ( T ) with respect to the ( w )-basis. We seek to construct the matrix ( C ) such that<br>$$
A = C^{-1} B C.
$$</p><p>Recall that given the ( v )-basis, any vector ( \mathbf{z} \in V ) can be written as an ( n \times 1 ) column vector as follows: there exist unique scalars ( a_1, \dots, a_n ) such that<br>$$
\mathbf{z} = a_1 \mathbf{v}_1 + \dots + a_n \mathbf{v}_n.
$$</p><p>We then represent ( \mathbf{z} ) with respect to the ( v )-basis as the column vector:<br>$$
\begin{pmatrix}
a_1 \
\vdots \
a_n
\end{pmatrix}.
$$</p><p>Similarly, there exist unique scalars ( b_1, \dots, b_n ) such that<br>$$
\mathbf{z} = b_1 \mathbf{w}_1 + \dots + b_n \mathbf{w}_n,
$$<br>which means that with respect to the ( w )-basis, the vector ( \mathbf{z} ) is represented as:<br>$$
\begin{pmatrix}
b_1 \
\vdots \
b_n
\end{pmatrix}.
$$</p><p>The desired matrix ( C ) is the one that satisfies the relationship:<br>$$
C A = B C.
$$</p><p>Determining when two matrices are similar is a result that frequently arises in mathematics and physics. Often, we need to select a coordinate system (a basis) to express anything, but the underlying mathematics or physics of interest is independent of the initial choice. The key question then becomes: <em>What is preserved when the coordinate system changes?</em> Similar matrices provide a starting point to understand these issues.</p><h2 id=eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors<a hidden class=anchor aria-hidden=true href=#eigenvalues-and-eigenvectors>#</a></h2><p>In the previous section, we observed that two matrices represent the same linear transformation under different choices of bases precisely when they are similar. However, this does not tell us how to choose a basis for a vector space so that a linear transformation has a particularly simple matrix representation. For instance, the diagonal matrix<br>$$
A =
\begin{pmatrix}
1 & 0 & 0 \
0 & 2 & 0 \
0 & 0 & 3
\end{pmatrix}
$$<br>is similar to the matrix<br>$$
B =
\begin{pmatrix}
1 & -5 & 15 \
1 & -1 & 84 \
-4 & -15 & 15
\end{pmatrix},
$$<br>but the simplicity of ( A ) is evident compared to ( B ). (Incidentally, it is not obvious that ( A ) and ( B ) are similar; I started with ( A ), chose a nonsingular matrix ( C ), and then used Mathematica software to compute ( C^{-1}AC ) to obtain ( B ). This was not immediately apparent but intentionally set up.)</p><p>One of the purposes of the following definitions of eigenvalues and eigenvectors is to provide tools for selecting good bases. However, there are many other reasons to understand eigenvalues and eigenvectors.</p><p>:::{.definition}<br>Let ( T: V \to V ) be a linear transformation. A nonzero vector ( \mathbf{v} \in V ) is an eigenvector of ( T ) with eigenvalue ( \lambda ), a scalar, if<br>$$
T(\mathbf{v}) = \lambda \mathbf{v}.
$$<br>For an ( n \times n ) matrix ( A ), a nonzero column vector ( \mathbf{x} \in \mathbb{R}^n ) is an eigenvector with eigenvalue ( \lambda ), a scalar, if<br>$$
A \mathbf{x} = \lambda \mathbf{x}.
$$<br>:::</p><p>Geometrically, a vector ( \mathbf{v} ) is an eigenvector of the linear transformation ( T ) with eigenvalue ( \lambda ) if ( T ) stretches ( \mathbf{v} ) by a factor of ( \lambda ).</p><p>For example, consider the matrix<br>$$
\begin{pmatrix}
-2 & 1 \
2 & 7
\end{pmatrix}.
$$<br>Here, ( \lambda = 2 ) is an eigenvalue, and ( \mathbf{v} ) is an eigenvector for the linear transformation represented by this ( 2 \times 2 ) matrix.</p><p>Fortunately, there is a simple way to describe the eigenvalues of a square matrix, allowing us to see that the eigenvalues of a matrix are preserved under a similarity transformation.</p><p>:::{.definition}<br>A scalar ( \lambda ) is an eigenvalue of a square matrix ( A ) if and only if ( \lambda ) is a root of the polynomial<br>$$
P(t) = \det(tI - A),
$$<br>where ( P(t) ) is called the characteristic polynomial of ( A ).<br>:::</p><p><strong>Proof</strong>: Suppose ( \lambda ) is an eigenvalue of ( A ), with eigenvector ( \mathbf{v} ). Then,<br>$$
A \mathbf{v} = \lambda \mathbf{v},
$$<br>which implies<br>$$
A \mathbf{v} - \lambda \mathbf{v} = 0.
$$<br>Introducing the identity matrix ( I ), we rewrite this as<br>$$
0 = (\lambda I - A) \mathbf{v}.
$$<br>Thus, the matrix ( \lambda I - A ) has a nontrivial kernel, ( \mathbf{v} ). By the key theorem of linear algebra, this occurs precisely when<br>$$
\det(\lambda I - A) = 0,
$$<br>which means ( \lambda ) is a root of the characteristic polynomial ( P(t) = \det(tI - A) ). Since these implications are reversible, the theorem is established.</p><p>:::{.theorem}<br>If ( A ) and ( B ) are similar matrices, then the characteristic polynomial of ( A ) is equal to the characteristic polynomial of ( B ).<br>:::</p><p><strong>Proof</strong>: For ( A ) and ( B ) to be similar, there must exist an invertible matrix ( C ) such that ( A = C^{-1}BC ). Then,<br>$$
\det(tI - A) = \det(tI - C^{-1}BC) = \det(C^{-1}(tI - B)C) = \det(C^{-1})\det(tI - B)\det(C) = \det(tI - B),
$$<br>using the property ( \det(C^{-1}C) = 1 = \det(C^{-1})\det(C) ).</p><p>Since similar matrices have the same characteristic polynomial, it follows that their eigenvalues must also be the same.</p><p>:::{.corollary}<br>The eigenvalues of similar matrices are identical. Thus, to determine if two matrices are not similar, check whether their eigenvalues differ. If they do, the matrices are not similar.</p><p>However, in general, identical eigenvalues do not imply that matrices are similar. For example, the matrices<br>$$
A =
\begin{pmatrix}
4 & 2 \
7 & 5
\end{pmatrix}
\quad \text{and} \quad
B =
\begin{pmatrix}
2 & 3 \
5 & 4
\end{pmatrix}
$$<br>have eigenvalues ( 1 ) and ( 2 ), but they are not similar. (This can be shown by assuming the existence of an invertible matrix ( C ) such that ( C^{-1}AC = B ) and then demonstrating that ( \det(C) = 0 ), which contradicts the invertibility of ( C ).)<br>:::</p><h3 id=eigenvalues-and-determinants>Eigenvalues and Determinants<a hidden class=anchor aria-hidden=true href=#eigenvalues-and-determinants>#</a></h3><p>Since the characteristic polynomial ( P(t) ) does not change under a similarity transformation, the coefficients of ( P(t) ) are also preserved. As these coefficients are polynomial functions (complex) of the entries of the matrix ( A ), we obtain special polynomials of the entries of ( A ) that are invariant under similarity transformations. One such coefficient, already seen in another form, is the determinant of ( A ), as highlighted in the following theorem. This theorem establishes a deeper connection between the eigenvalues of ( A ) and the determinant of ( A ).</p><p>:::{.theorem}<br>Let ( \lambda_1, \lambda_2, \dots, \lambda_n ) be the eigenvalues (counted with multiplicity) of a matrix ( A ). Then,<br>$$
\det(A) = \lambda_1 \cdot \lambda_2 \cdot \dots \cdot \lambda_n.
$$<br>:::</p><p>Before proving this theorem, we need to clarify the concept of counting eigenvalues &ldquo;with multiplicity.&rdquo; The difficulty arises because a polynomial can have a root that must be counted more than once. For example, the polynomial ( (z - 2)^2 ) has a single root ( 2 ), but we count it twice. This situation is particularly relevant for the characteristic polynomial.</p><p>For instance, consider the matrix<br>$$
A =
\begin{pmatrix}
5 & 0 & 0 \
0 & 5 & 0 \
0 & 0 & 4
\end{pmatrix},
$$<br>which has the characteristic polynomial ( P(t) = (t - 5)(t - 5)(t - 4) ). Here, we list the eigenvalues as ( 4, 5, ) and ( 5 ), counting the eigenvalue ( 5 ) twice.</p><p><strong>Proof</strong>: Since the eigenvalues ( \lambda_1, \lambda_2, \dots, \lambda_n ) are the roots of the characteristic polynomial ( \det(tI - A) ), we can write<br>$$
\det(tI - A) = (t - \lambda_1)(t - \lambda_2) \cdots (t - \lambda_n).
$$<br>Setting ( t = 0 ), we obtain<br>$$
(-1)^n \lambda_1 \lambda_2 \cdots \lambda_n = \det(-A).
$$<br>In the matrix ( -A ), each column of ( A ) is multiplied by ( -1 ). Using the linearity of the determinant, we can factor out ( -1 ) from each column, yielding<br>$$
\det(-A) = (-1)^n \det(A).
$$<br>Substituting, we have<br>$$
(-1)^n \lambda_1 \lambda_2 \cdots \lambda_n = (-1)^n \det(A),
$$<br>and thus,<br>$$
\det(A) = \lambda_1 \cdot \lambda_2 \cdots \lambda_n.
$$</p><p>To find a &ldquo;good&rdquo; basis for representing a linear transformation, we measure &ldquo;goodness&rdquo; by how close the matrix is to being diagonal. Here, we focus on a special but common class of matrices: symmetric matrices. A matrix ( A ) is symmetric if ( A = (a_{ij}) ) satisfies ( a_{ij} = a_{ji} ), meaning the element in the ( i )-th row and ( j )-th column equals the element in the ( j )-th row and ( i )-th column.</p><p>For example:<br>$$
\begin{pmatrix}
5 & 3 & 4 \
3 & 5 & 2 \
4 & 2 & 4
\end{pmatrix}
$$<br>is symmetric, but<br>$$
\begin{pmatrix}
5 & 6 & 2 \
2 & 5 & 18 \
3 & 3 & 4
\end{pmatrix}
$$<br>is not.</p><p>:::{.theorem}<br>If ( A ) is a symmetric matrix, then there exists a matrix ( B ) similar to ( A ) that is diagonal. Moreover, the entries along the diagonal of ( B ) are precisely the eigenvalues of ( A ).<br>:::</p><p><strong>Proof</strong>: The proof relies on showing that the eigenvectors of ( A ) form a basis such that ( A ) becomes diagonal in this basis. We assume that ( A )&rsquo;s eigenvalues are distinct since technical difficulties arise when eigenvalues have multiplicity.</p><p>Let ( v_1, v_2, \dots, v_n ) be the eigenvectors of ( A ) with corresponding eigenvalues ( \lambda_1, \lambda_2, \dots, \lambda_n ). Form the matrix<br>$$
C =
\begin{pmatrix}
v_1 & v_2 & \dots & v_n
\end{pmatrix},
$$<br>where the ( i )-th column of ( C ) is the column vector ( v_i ). We will show that the matrix ( C^{-1}AC ) satisfies our theorem. Specifically, we aim to demonstrate<br>$$
C^{-1}AC = B,
$$<br>where ( B ) is the desired diagonal matrix.</p><p>Define ( B ) as<br>$$
B =
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \
0 & \lambda_2 & \cdots & 0 \
\vdots & \vdots & \ddots & \vdots \
0 & 0 & \cdots & \lambda_n
\end{pmatrix}.
$$<br>The diagonal matrix ( B ) is the only matrix satisfying ( Be_i = \lambda_i e_i ) for all ( i ), where ( e_i ) is the standard basis vector. Observe that ( Ce_i = v_i ) for all ( i ). Thus,<br>$$
C^{-1}ACe_i = C^{-1}Av_i = C^{-1}(\lambda_i v_i) = \lambda_i C^{-1}v_i = \lambda_i e_i,
$$<br>proving that<br>$$
C^{-1}AC = B.
$$</p><p>This result is not the end of the story. For nonsymmetric matrices, there are other canonical forms to find &ldquo;good&rdquo; similar matrices, such as the Jordan canonical form, upper triangular form, and rational canonical form.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://jadrk040507.github.io/skope/methodology/public/post/2020-12-01-r-rmarkdown/><span class=title>Next »</span><br><span>Hello R Markdown</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on x" href="https://x.com/intent/tweet/?text=Linear%20Algebra&amp;url=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f&amp;title=Linear%20Algebra&amp;summary=Linear%20Algebra&amp;source=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f&title=Linear%20Algebra"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on whatsapp" href="https://api.whatsapp.com/send?text=Linear%20Algebra%20-%20https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on telegram" href="https://telegram.me/share/url?text=Linear%20Algebra&amp;url=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Linear Algebra on ycombinator" href="https://news.ycombinator.com/submitlink?t=Linear%20Algebra&u=https%3a%2f%2fjadrk040507.github.io%2fskope%2fmethodology%2fpublic%2fpost%2f2024-12-11-linear-algebra%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://jadrk040507.github.io/skope/methodology/public/>ExampleSite</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>